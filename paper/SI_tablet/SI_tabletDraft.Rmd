---
title: "Reaction times in processing scalar implicatures"
bibliography: biblibrary.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Rose M. Schneider} \\ \texttt{rschneid@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
    Children's trouble with scalar implicatures -- inferences from a weaker lexicalized description that a stronger alternative is true -- is a puzzle in pragmatic development. Previous research indicates that children's failures in processing scalar implicatures may be rooted in an unestablished quantifier scale, with children who struggle with the quantifier "some" being unable to contrast different quantifiers to make the implicature. However, the source of this failure is unclear. Here, we explore reaction time as a measure of processing for scalar implicatures (and reasoning about salient alternatives, such as "none"). In our analyses, we explore overall performance and reaction time patterns across development, finding that increased reaction times and accuracy for the quantifiers "some" and "none." Motivated by these findings, we use a Drift Diffusion Model to explore the relationship between accuracy and reaction time in processing both scalar implicatures, and the quantifiers "some" and "none" more broadly. Overall, we find evidence that while children's performance in scalar implicature tasks is hindered by absent quantifier knowledge, their success also requires additional processing when reasoning about that quantifier scale.
    
keywords:
    Pragmatics; development; language.
    
output: cogsci2016::cogsci_paper
---
```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=TRUE, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(reshape)
# library(plyr)
library(lme4)
library(dplyr)
library(stringr)
library(tidyr)
# library(markdown)
library(directlabels)
library(magrittr)
# library(bootstrap)
library(RCurl)
library(langcog)
# library(RColorBrewer)
library(diptest)
library(RWiener)
# library(ppcor)
theme_set(theme_bw())
```

```{r data}
df <- read.csv("tmp.csv")
```

```{r data cleaning}
df %<>%
  dplyr::mutate(age = as.numeric(age))%>%
  dplyr::mutate(resp = factor(correct, levels=c("Y","N"), labels=c("upper","lower")), 
         q = rt/1000)%>%
  dplyr::filter(selection_type != "someall", na.rm=TRUE) %>% #this is filtering out the strange repeats
  dplyr::filter(selection_type != "allall", na.rm=TRUE) %>%
  dplyr::filter(selection_type != "allallall", na.rm=TRUE)%>%
  dplyr::filter(age <= 6.5, na.rm = TRUE)%>%
  dplyr::filter(english >= 0.75, na.rm=TRUE) %>%
  dplyr::mutate(age_round = round(age, digits = 2))%>%
  dplyr::mutate(agesplit = cut(age_round, breaks=c(3, 3.5, 4, 4.5, 5, 6.5)),
         agesplit = factor(agesplit,
                           labels=c("3-3.5 years", "3.5-4 years", 
                                    "4-4.5 years", "4.5-5 years", 
                                    "5+ years")))%>%
  dplyr::filter(agesplit != "NA", na.rm = TRUE)

#for means
df$correct %<>%
  str_replace("Y", 1)%>%
  str_replace("N", 0)

df %<>%
  dplyr::mutate(correct = as.numeric(correct))

#renaming for better graphs
df$trial_type %<>%
  str_replace("all", "All")%>%
  str_replace("some", "Some")%>%
  str_replace("none", "None")
  
df$selection_type %<>%
  str_replace("all", "All")%>%
  str_replace("some", "Some")%>%
  str_replace("none", "None")

x <- unique(df$sub_id)

pre_cut <- df
  
```

```{r exclusions}
# Exclusions

df1 <-  subset(df, sub_id != "11716_14" & sub_id != "11316_2" 
               & sub_id != "121815_7" & sub_id != "1616_6" 
               & sub_id != "TM001" & sub_id != "TM003" & sub_id != "TM004" &
                 sub_id != "12416_9" & sub_id!= "TM011") #excluded due to consent issue, and low number of trials, and parental interference

df <- df1


# df %<>%
#   dplyr::filter(sub_id != "11716_14" & sub_id != "11316_2" & sub_id != "12016_1" 
#                 & sub_id != "12016_2" & sub_id != "12016_4" & sub_id != "12016_8" 
#                 & sub_id != "12016_9" & sub_id != "11016_5" & sub_id != "11716_10" 
#                 & sub_id != "1616_3" & sub_id != "11016_12" & sub_id != "121815_5" 
#                 & sub_id != "121815_6" & sub_id != "121815_7" & sub_id != "1616_7"
#                 & sub_id != "12216_12" & sub_id != "12116_5" & sub_id != "12216_8")%>% # for english < 75%
#   dplyr::filter(sub_id != "1616_6", na.rm = TRUE)%>%#broken iPad
#   dplyr::filter(sub_id != "1616_4", na.rm = TRUE)%>%
#   dplyr::filter(sub_id != "TM001" & sub_id != "TM002" & sub_id != "TM003" & sub_id != "TM004" & sub_id != "TM005" & sub_id != "TM006" & sub_id != "TM007", na.rm = TRUE) #just until tamara's data is complete
# 
# df %<>%
#   dplyr::filter(sub_id != "NA")%>%
#   dplyr::filter(age != "NA")

pre_cut <- df
```

```{r rt exclusion}
# qplot(rt, data = df)

#There are some really crazy rts - exclude everything above 15s
df$clean.rt <- df$rt
df$clean.rt[df$rt > 15000] <- NA
mlog <- mean(log(df$clean.rt), na.rm=TRUE)
sdlog <- sd(log(df$clean.rt), na.rm=TRUE)

# qplot(clean.rt, data=df)

# qplot(log(clean.rt), 
#       fill = log(clean.rt) > mlog + 3*sdlog | log(clean.rt) < mlog - 3*sdlog,
#       data = df)

df$clean.rt[log(df$clean.rt) > mlog + 3*sdlog | 
              log(df$clean.rt) < mlog - 3*sdlog] <- NA

#filter df to exclude NAs (RT cleaned)
df %<>%
  filter(clean.rt != "NA")

#how much data do we lose in this data cleaning?
total_loss <- as.numeric(nrow(pre_cut) - nrow(df))
percentage <- as.numeric((total_loss/nrow(df))*100)

```

```{r subs}
#Proportion correct by sub_id (initial analysis)
# ms <- df %>%
#   group_by(trial_type, sub_id) %>%
#   multi_boot_standard("correct", na.rm=TRUE)

# ggplot(data = ms, 
#        aes(x=trial_type, y=mean)) +
#   geom_bar(stat="identity", position = "dodge") + ylab("Average correct") + xlab("Trial type") + facet_wrap(~sub_id)
```

```{r hist of responses}
#What does a histogram of responses look like?
# ms <- df %>%
#   group_by(sub_id, trial_type) %>%
#   summarise(correct = sum(correct))
  
# ggplot(ms, aes(x = correct)) + 
#   geom_histogram(binwidth = 1) +
#   facet_wrap(~trial_type)
```
# Introduction
In listening to language, we are presented with a large amount of information to process in a very short period of time. For the most part, we're successful communicators, and can integrate linguistic information quickly and efficiently. How do we develop this ability, particularly in challenging pragmatic contexts where meaning is not stated, but implied? When we do make errors, where does the language comprehension process break down? We know that early in development, children often stumble in comprehending language. A particularly salient and well-researched example of this pragmatic development is preschoolers' failures in making scalar implicatures. What can children's difficulties in this case reveal about breakdowns in language comprehension more broadly?

In comprehending language, as listeners we have available to us not only a speaker's verbalized statement, but also the knowledge of what the speaker *could* have said. In fact, listeners frequently go beyond the literal sense of utterances to infer a speaker's intended meaning. In the case of *pragmatic implicatures* [@grice1975logic], a weaker literal description can imply that a stronger alternative is true. Thus, an adult listener would strongly infer from the statement ``I enjoyed *some* of my winter break'' that some (but not *all*) of my break was pleasant. This *scalar implicature* (SI) relies heavily on a knowledge of the relevant lexical alternatives in the quantifier scale $<$none, some, all$>$, as a listener must be able to contrast these alternatives in computing the implicature. While scalar implicatures are easily comprehended by adults, they pose a pragmatic challenge to children until fairly late in development [@horowitz2015;@katsos2011;@papafragou2003]. What is the source of children's difficulties with scalar implicatures?

In contrast to adults' spontaneous processing of such pragmatic implicatures, children display marked and striking failures in such tasks. For example, when judging a scene in which three of three horses have jumped over a fence, preschoolers are likely to endorse the statement "*All* of the horses jumped over the fence" as felicitous, rather than the appropriate statement "*Some* of the horses jumped over the fence" [@papafragou2003]. Across different studies, however, children exhibit varying performance, depending on the paradigm, syntactic construction of the implicature prompts, access to visual and lexical alternatives, and age [@guasti2005; @noveck2001; @papafragou2003; @papafragou2004; @horowitz2015]. Making inferences from these various datasets about the source of children's failures in making scalar implicatures is made difficult by various measures and tasks uses.

In an attempt to reconcile these various accounts, Horowitz and Frank [-@horowitz2015] designed a simple referent selection paradigm that could be used across a broad age range (3--5 years) to explore lexicalized (scalar) implicatures. In this task, children saw three book covers, each featuring four familiar objects (Figure \ref{fig:image}), and the experimenter described a book using either a scalar (quantifier) description (e.g., "On the cover of my book, *none/some/all* of the pictures are cats."). Children's responses were scored as correct if they selected the book consistent with the quantifier description. 

```{r image, fig.pos = "b", fig.align='center', fig.width=3, fig.height=3, fig.cap = "Example trial stimuli used in Horowitz and Frank (2015)."}
img <- png::readPNG("figs/implicatures_demo_letters.png")
grid::grid.raster(img)
```

Using this paradigm, Horowitz and Frank found children struggled with the quantifiers "some" and "none" in relation to "all", although performance increased across development. Intriguingly, they found that performance between these two quantifiers was strongly bimodal and correlated: Children who failed on trials with the quantifier "some" similarly struggled with "none," and vice versa.

Children's failures in computing scalar implicatures, and their related performance processing the quantifiers "some" and "none" presented two hypotheses for sources of developmental difficulty in this task, namely, lack of quantifier knowledge and developing inhibitory control [@horowitzInPrep]. Making a scalar implicature requires familiarity with and ability to contrast alternatives on the quantifier scale $<$*none -- some -- all*$>$; if children's quantifier knowledge is absent or unestablished, it might lead to failures in making an implicature. Alternatively, children may have complete quantifier knowledge, but are unable to inhibit an impulse to choose a more salient alternative. 

Horowitz et al. explored these two hypotheses in an individual differences task, running their scalar implicature [@horowitz2015] task in conjunction with a quantifier-knowledge [@barner2009] and inhibitory control [@zelazo2006] tasks. Overall, they found that children's scalar implicature performance was strongly correlated with quantifier knowledge, even when controlling for age. While inhibitory control was strongly correlated with age, it was not related to children's performance on the SI task [@horowitzInPrep]. 

While the ability to compute a scalar implicature seems to be closely related to knowledge of the complete quantifier scale (rather than pure inhibitory control), it is likely that children's failures have several underlying causes. In particular, an unexplored (and more nuanced) measure of inhibitory control is the response latency associated with making an implicature. Here, we explore children's response times in Horowitz and Frank's [-@horowitz2015] task as a measure of inhibitory control. To obtain accurate reaction times, we adapted the task for an iPad, and expanded the sample size. 

In our analyses, we explore overall accuracy and patterns of performance, as in [@horowitz2015; @horowitzInPrep.], and find that children not only struggle in making a scalar implicature, but also grapple with the quantifier "none" until fairly late in development. In examining reaction time patterns across all quantifier types, we find a speed-accuracy tradeoff associated with these two quantifiers, even later in development. Finally, we use a Drift Diffusion Model to our data, and find that longer response latencies are predictive of success in computing a scalar implicature, and understanding the quantifier "none." Overall, our findings indicate that while quantifier knowledge is a key factor in successfully computing scalar implicatures, using this information to successfully compute a scalar implicature is particularly difficult, and requires additional processing.

# General Methods

In this study, we adapted a Scalar Implicature paradigm developed by Horowitz and Frank [-@horowitz2015] for the iPad. In addition to capturing detailed reaction time data, this version included more trials, and standardized prosody across all trials, in addition to a completely randomized design. \footnote{All of our data, processing, and analysis code can be viewed in the version control repository for this paper at: https://github.com/rosemschneider/SI\_tablet.} 

### Participants

```{r descriptives}
num_subjects <- df %>%
  select(sub_id)%>%
  distinct()

num_trials <- df %>%
  group_by(sub_id, agesplit)%>%
  summarise(n = n())

num_kids <- df %>% 
    dplyr::select(sub_id, agesplit) %>%
    dplyr:: distinct()%>%
    dplyr::group_by(agesplit)%>%
    dplyr::summarize(n=n())%>%
    dplyr::mutate(total.n = sum(n))

num_young3s <- num_kids %>%
  filter(agesplit == "3-3.5 years")

num_old3s <- num_kids %>%
  filter(agesplit == "3.5-4 years")

num_young4s <- num_kids %>%
  filter(agesplit == "4-4.5 years")

num_old4s <- num_kids %>%
  filter(agesplit == "4.5-5 years")

num_5s <- num_kids %>%
  filter(agesplit == "5+ years")

sstats <- df %>%
    dplyr::group_by(agesplit)%>%
    dplyr::summarise(mean = round(mean(age), digits = 2), median = round(median(age), digits = 2), sd = round(sd(age), digits = 2))

s_young3 <- sstats %>%
  filter(agesplit == "3-3.5 years")

s_old3 <- sstats %>%
  filter(agesplit == "3.5-4 years")

s_young4 <- sstats %>%
  filter(agesplit == "4-4.5 years")

s_old4 <- sstats %>%
  filter(agesplit == "4.5-5 years")

s_5s <- sstats %>%
  filter(agesplit == "5+ years")
```

`r num_young3s$total.n` out of a planned sample of 120 was recruited from Bing Nursery School at Stanford University and the Children's Discovery Museum in San Jose. Participants ranged in age from 3--6.5 years: `r num_young3s$n` 3--3.5-year-olds (M = `r s_young3$mean`, median = `r s_young3$median`, SD = `r s_young3$sd`); `r num_old3s$n` 3.5--4-year-olds (M = `r s_old3$mean`, median = `r s_old3$median`, SD = `r s_old3$sd`); `r num_young4s$n` 4--4.5-year-olds (M = `r s_young4$mean`, median = `r s_young4$median`, SD = `r s_young4$sd`); `r num_old4s$n` 4.5--5-year-olds (M = `r s_old4$mean`, median = `r s_old4$median`, SD = `r s_old4$sd`); and `r num_5s$n` 5--6.5-year-olds (M = `r s_5s$mean`, median = `r s_5s$median`, SD = `r s_5s$sd`). Two additional children were run, but were excluded as out of the age range. Based on [@horowitz2015; @horowitzInPrep], the initially planned sample size was 96 children from 3--5 years. After collecting data from 57 participants, however, we observed significantly lower performance on critical trials across all age groups, indicating that the iPad adaptation of the scalar implicature task was slightly more challenging for all children. With this extended developmental trajectory in mind, we decided to include an older age group of twenty-four 5--6.5-year-olds. 

### Stimuli

The general format of the task was identical to [@horowitz2015], with the exception of added items for additional trials. The study was programmed in HTML, CSS, and JavaScript, and displayed to children on full-sized iPad. Each trial displayed three book covers, each containing a set of four familiar objects (Figure \ref{fig:image}). Each trial allowed 2.5s for children to visually inspect the three book covers, before the experiment played the trial prompt (e.g., "On the cover of my book, *none* of the pictures are cats."). Each trial was completely randomized, with the exception that similar items were displayed together (e.g., food, clothing). Each session involved 30 trials, with 10 trials per quantifier-type ("all", "some", and "none"). In our randomization, quantifier triad order, items (within category), target item, and quantifier, were randomized for all participants. 

###Procedure

Sessions took place individually in a small testing room away from either the museum or the nursery school. Each session began with the child playing the "dot game," which required them to press dots on the iPad screen as fast as they could. This game was included to familiarize children with the iPad. 

After children finished the dot game, the experimenter introduced them to "Hannah," a cartoon character who wanted to play a guessing game with her books. The experimenter explained that Hannah would show the child three books, and would give the child *one* hint about which book she was thinking of. The experimenter emphasized that Hannah would only give one hint, so they had to listen carefully. Children then saw a practice trial with three books featuring a refrigerator, a TV, and a couch. After 2.5s, a female voice said "On the cover of my book there's a TV." Once children correctly made their selection, a green box appeared around the selection. Children moved trials along at their own pace by pressing a green button that appeared after they had made their selection. 

Reaction times were measured from the onset of the target word. Each audio clip used the same three frames (e.g., "On the cover of my book, *some* of the pictures...") so that prodosdy was emphasized equally across all trials. Across all trials, the average length of each audio clip (including target item phrase, e.g., "...are cats") was approximately 6s. In all, there were 270 different target items and audio clips.

Children could only make one selection. If a child was not paying attention, or if she did not hear Hannah's prompt, the experimenter repeated it, matching the original prosody. 

#Results

In analyzing the results, we excluded any trials in which reaction time exceeded thirty seconds, which indicated that the child had missed the prompt, or was not paying attention. After this initial cut, we excluded responses outside three standard deviations of the log of the reaction time mean. This cleaning process resulted in a data loss of `r total_loss` trials (`r round(percentage, 2)`%).

##Accuracy
```{r overall_acc, fig.pos = "h", fig.width=3.2, fig.height=2, fig.cap = "Children's overall accuracy for each quantifier type. Bars show mean performance for each age group. Error bars are 95 percent confidence intervals computed by non-parametric bootstrap."}
ms <- df %>%
  group_by(trial_type, agesplit)%>%
  multi_boot_standard("correct", na.rm=TRUE)

ms$trial_type %<>%
  str_replace("all", "All")%>%
  str_replace("none", "None")%>%
  str_replace("some", "Some")

quartz()
ggplot(data = ms, 
       aes(x=trial_type, y=mean, fill= agesplit)) +
  geom_bar(stat="identity", position = position_dodge()) +
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .4,
                  show.legend = FALSE,
                 position=position_dodge(.9)) +
  ylab("Proportion correct") + 
  xlab("Trial Type") +
  theme_bw(base_size = 8) + theme(axis.title.x = element_text(size=8),
           axis.title.y  = element_text(size=8)) + 
  scale_fill_solarized(name = "Age") + theme(legend.position=c(.8,.8)) + theme(legend.key.size=unit(.2, "cm")) + theme(legend.title=element_text(size=5)) + theme(legend.text=element_text(size=5))
```


```{r t tests}
t_tests <- aggregate(correct ~ trial_type + agesplit +  sub_id, data=df, mean)

#tests against quantifiers

#3-3.5s all and none
young3_allnone <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="All")$correct)
#all and some
young3_allsome <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="All")$correct)
#some and none
young3_somenone <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="Some")$correct)

#3.5s-4s 
old3_allnone <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="All")$correct)
old3_allsome <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="All")$correct)
old3_somenone <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="Some")$correct)

#4-4.5s
young4_allnone <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="All")$correct)
young4_allsome <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="All")$correct)
young4_somenone <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="Some")$correct)

#4.5-5s
old4_allnone <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="All")$correct)
old4_allsome <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="All")$correct)
old4_somenone <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="Some")$correct)

#5s
fives_allnone <- t.test(subset(t_tests, agesplit=="5+ years" & trial_type=="None")$correct, subset(t_tests, agesplit=="5+ years" & trial_type=="All")$correct)
fives_allsome <- t.test(subset(t_tests, agesplit=="5+ years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="5+ years" & trial_type=="All")$correct)
fives_somenone <- t.test(subset(t_tests, agesplit=="5+ years" & trial_type=="None")$correct, subset(t_tests, agesplit=="5+ years" & trial_type=="Some")$correct)
```
In our first planned analysis, we explored children's overall patterns of accuracy for each quantifier type. Figure \ref{fig:overall_acc} shows children's performance for each quantifier type. For each age group, we saw significantly lower accuracy for the quantifiers "some" and "none" in comparison to "all" in independent t-tests within each age group ($p$ < .01 for all tests). This replication of previous results using this paradigm [@horowitz2015; @horowitzInPrep] indicates that our iPad adaptation of this task is an appropriate measure.

```{r comparison}
d_si <- read.csv("experiment3.csv") #pull in aggregated data from previous analysis

t_tests %<>%
  spread(trial_type, correct)
#only for some and none trials, because we don't find that the all trials are much affected by the transfer to the iPad
#3-3.5 years
list <- c(
t.test(subset(t_tests, agesplit=="3-3.5 years")$None, subset(d_si, Age=="3-3.5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="3-3.5 years")$Some, subset(d_si, Age=="3-3.5 years")$Some, var.equal = TRUE),

#3.5-4s
t.test(subset(t_tests, agesplit=="3.5-4 years")$None, subset(d_si, Age=="3.5-4 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="3.5-4 years")$Some, subset(d_si, Age=="3.5-4 years")$Some, var.equal = TRUE),

#4-4.5s
t.test(subset(t_tests, agesplit=="4-4.5 years")$None, subset(d_si, Age=="4-4.5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="4-4.5 years")$Some, subset(d_si, Age=="4-4.5 years")$Some, var.equal = TRUE),

#4.5-5s
t.test(subset(t_tests, agesplit=="4.5-5 years")$None, subset(d_si, Age=="4.5-5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="4.5-5 years")$Some, subset(d_si, Age=="4.5-5 years")$Some, var.equal = TRUE))
```

Because our adaptation relies strictly on verbal communication, however, it may prove more difficult for children, resulting in lowered performance across all age groups. We found that children lose some communicative power when relying only on linguistic information in this task. Children aged 3--5 years perform significantly lower on "some" (implicature) trials in this task in comparison with [@horowitzInPrep] in independent t-tests ($p$ < .05 for all tests). We believe that the lower performance on this task is the result of a more challenging experimental context, rather than developmental pragmatic failures. 

###Statistical modeling

```{r accuracy model}
lm <- summary(glmer(correct ~ age * trial_type + 
               (trial_type | sub_id), 
             family = "binomial", data = df))
```

In exploring children's signficantly lower performance on "some" and "none" trials, we ran a logistic mixed effects model predicting correct response as an interaction of age and trial type, with random effects of trial type and participant. \footnote{Mixed effects model fit in R using the lme4 package. The model specifications were as follows: \texttt{correct ~ age * trial type + (trial type | subject id)}.} We found that performance was significantly lower on "some" ($\beta$ = `r round(lm$coefficients[4], 2)`, $p$ < .0001) and "none" trials ($\beta$ = `r round(lm$coefficients[3], 2)`, $p$ < .0001). We also found a signficant interaction between age and trial type on "none" trials($\beta$ = `r round(lm$coefficients[5], 2)`, $p$ < .0001), indicating that children's performance with this difficult quantifier increased with age. 

###Correlation between "some" and "none"

```{r diptest, fig.pos = "h", fig.width=3.2, fig.height=2, fig.cap = "Frequency histogram of correct responses for each trial type, across all participants."}
ms <- df %>%
  group_by(sub_id, trial_type) %>%
  multi_boot_standard("correct", na.rm=TRUE)

diptest_all <- diptest::dip.test(filter(ms, trial_type == "All")$mean)
diptest_some <- diptest::dip.test(filter(ms, trial_type == "Some")$mean)
diptest_none <- diptest::dip.test(filter(ms, trial_type == "None")$mean)

ms <- df %>%
  group_by(sub_id, trial_type) %>%
  summarise(correct = sum(correct))
  
quartz()
ggplot(ms, aes(x = correct, fill=trial_type)) + 
  geom_histogram(binwidth = 1) + theme_bw(base_size = 8) + xlab("Total correct responses per participant") + ylab("Frequency") + facet_wrap(~trial_type) + theme(legend.position="none") + langcog::scale_fill_solarized()
```

```{r correlation}
ms.acc <- df %>%
  dplyr::group_by(trial_type, agesplit, sub_id) %>%
  multi_boot_standard("correct", na.rm = TRUE) %>%
  dplyr::select(-ci_lower, -ci_upper)%>%
  spread(trial_type, mean)

# ms.acc.p <- df %>%
#   dplyr::group_by(trial_type, age, sub_id) %>%
#   multi_boot_standard("correct", na.rm = TRUE) %>%
#   dplyr::select(-ci_lower, -ci_upper)%>%
#   spread(trial_type, mean)%>%
#   dplyr::filter(Some != "NA" & None != "NA")%>%
#   dplyr::mutate(age = as.numeric(age))

#ggcorplot(ms.acc %>% filter(complete.cases(ms.acc)) %>% dplyr::select(None, Some, All))

#correlation test
sn_cor <- cor.test(ms.acc$Some, ms.acc$None)

r_corr <- round(sn_cor$estimate, digits = 2)
p_corr <- round(sn_cor$p.value, digits = 6)
# sn_pcor <- pcor.test(ms.acc.p$Some, ms.acc.p$None, ms.acc.p$age)
```

In previous research, a strong correlation has been found on children's performance with the quantifiers "some" and "none" [@horowitz2015; @horowitzInPrep]. Once again, we found correlated performance between these two quantifiers ($r$ = `r r_corr`, $p$ < .001). In running Hartigan's diptest for bimodality on these two quantifiers, we found significant bimodal distributions for "some" ($D$ = `r round(diptest_some$statistic, 2)`, $p$ < `r round(diptest_some$p.value, 3)`) and "none" trials ($D$ = `r round(diptest_none$statistic, 2)`, $p$ < .00001). While we also found that Hartigan's diptest indicated that performance in "all" trials was not signficantly unimodal ($D$ = `r round(diptest_all$statistic, 2)`, $p$ < .00001), this is most likely due to the large number of participants and trials in our study, as performance on "all" trials (Figure \ref{fig:diptest}) shows strong evidence of unimodality. These values indicate that while children's accuracy is significantly worse on "some" and "none" trials, their performance is consistent, and that they make their responses in meaningful manner over the course of the study. 

##Reaction time analyses

A previously-unexplored aspect of children's ability to compute scalar implicatures is their reaction time on these kinds of pragmatic tasks. Huang and Snedeker [-@huang2009] explored eye-movements as a measure of online pragmatic processing; response latencies in behavioral implicature data, however, remainly largely unknown. We hypothesized that children's reaction times on this task may be a measure of the processing involved in comprehending the quantifiers "some" and "none." It is possible that this more nuanced measure of inhibitory control may provide a clue as to the nature of children's correlated struggles with these terms. In recording reaction times, we began recording from the onset of the target nouns, and measured in milliseconds. Here, we explore overall trends in reaction times across this task, and the relationship between response latencies and accuracy on this task.

###Developmental reaction time distribution\

```{r rt_spread}

# ms <- df %>%
#   group_by(agesplit, trial_type) %>%
#   summarise(m = mean(clean.rt, na.rm=TRUE))
# 
# df$trial_type %<>%
#   str_replace("all", "All")%>%
#   str_replace("some", "Some")%>%
#   str_replace("none", "None")
# 
# quartz()
# ggplot(df, aes(x = clean.rt)) + 
#   geom_histogram() + 
#   facet_grid(agesplit ~ trial_type) + ylab("Count") + 
#   xlab("Reaction Time (ms)") +
#   geom_vline(data = ms, aes(xintercept = m), col = "red", lty = 2)

```

```{r rt summary stats}
rt_sstats <- df %>%
  gather(measure, rt, rt, clean.rt) %>%
  group_by(trial_type, measure) %>%
  summarise(mean = mean(rt, na.rm=TRUE), 
            sd = sd(rt, na.rm=TRUE), 
            max = max(rt, na.rm=TRUE), 
            min = min(rt, na.rm=TRUE), 
            median=median(rt, na.rm=TRUE)) 
```

```{r rt corr}
#correlation between reaction time and age
age_corr <- cor.test(df$age, df$rt)
```
Figure \ref{fig:rt_spread} shows the distribution of reaction times for each quantifier, faceted by age group. Overall, we found that reaction time was negatively correlated with age (*r* = `r round(age_corr$estimate, 2)`, *p* < .0001). In exploring the relationship between accuracy and reaction time, we found preliminary evidence of a speed-accuracy tradeoff across all trial types (Figure \ref{fig:density}) shows the density of reaction times for correct and incorrect selections for each quantifier type. 

```{r density, fig.env = "figure*", fig.pos = "t", fig.width=6, fig.height=3, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Density plots of reaction times for correct and incorrect responses on each trial type, split by age."}
trialtypes <- c("all", "some", "none")

df$resp1 <- df$resp %>%
  str_replace("lower", "Incorrect")%>%
  str_replace("upper", "Correct")

#for easier to read graph
df$resp1 <- factor(df$resp1, levels=c("Incorrect", "Correct"))

quartz()
ggplot(df, aes(x=q)) + 
  geom_density(aes(fill=resp1), alpha=0.3) +
  facet_grid(agesplit ~ trial_type, scales = "free") + 
  theme_bw(base_size = 8) + theme(axis.title.x = element_text(size=8),
           axis.title.y  = element_text(size=8)) + 
          scale_fill_solarized(name = "Response") +
          xlab("Response time (s)") + ylab("Density of responses")
```

```{r rts over trial}
# ggplot(df, aes(x = trial_num, y = clean.rt)) + 
#   geom_point() + 
#   geom_smooth(method = "lm", formula = y ~ log(x)) + 
#   facet_grid(agesplit ~ trial_type)
```

```{r rt corr_1}
ms.rt <- df %>%
  dplyr::group_by(trial_type, agesplit, sub_id) %>%
  dplyr::summarise(rt = mean(clean.rt, na.rm=TRUE)) %>%
  spread(trial_type, rt) 
# 
# ggcorplot(ms.rt %>% filter(complete.cases(ms.rt)) %>% dplyr::select(None, Some, All))

allsome_corr <- cor.test(ms.rt$All, ms.rt$Some)
allnone_corr <- cor.test(ms.rt$All, ms.rt$Some)
somenone_corr <- cor.test(ms.rt$Some, ms.rt$None)
```


Figure \ref{fig:density} also shows evidence for increased response latencies associated with the quantifiers "some" and "none," especially early in development, reflecting the difficulty in processing these terms. Overall, we found that children were largely consistent in their performance across the course of the study in reaction time correlations between "all" and "some" (*r* = `r round(allsome_corr$estimate, 2)`, *p* < .0001), "all" and "none" (*r* = `r round(allnone_corr$estimate, 2)`, *p* < .0001), and "some" and "none" trials (*r* = `r round(somenone_corr$estimate, 2), *p* < .0001). 

###Statistical modeling
```{r rt model}
rt_lm <- summary(lmer(log(rt) ~ scale(age, scale=FALSE) * log(trial_num) + 
               scale(age, scale=FALSE) * trial_type + 
               (trial_type | sub_id), 
             data = df))

# rt_lm <- summary(lmer(log(rt) ~ age * log(trial_num) + 
#                age * trial_type + 
#                (trial_type | sub_id), 
#              data = df))
```

We next turned to the relationship betwen age, reaction time, and accuracy. Our initial hypothesis was that computing quantifiers is pragmatically difficult, and doing so successfully may require additional processing time. In exploring this, we ran a planned linear mixed effects model predicting response time as an interaction of age, trial number, and trial type with a random effect of trial type. \footnote{Mixed effects model fit in R using the lme4 package. The model specifications were as follows: \texttt{log(reaction time) ~ scale(age) * log(trial number) + scale(age) * trial type + (trial type | subject id)}. We calculated \emph{p} values by treating the \emph{t} statistic as if it were a \emph{z} statistic [@barr2013].} We found a main effect of trial number, with reaction times decreasing over the course of the study (($\beta$ = `r round(rt_lm$coefficients[3], 2)`, $p$ < .00001)), but found that reaction times significantly increased on "none" ($\beta$ = `r round(rt_lm$coefficients[4], 2)`, $p$ < .00001), and "some" trials ($\beta$ = `r round(rt_lm$coefficients[5], 2)`, $p$ < .00001). Interestingly, we also found an interaction between age and trial type, such reaction time on "none" and "some" trials increased with age ("None": ($\beta$ = `r round(rt_lm$coefficients[6], 2)`, $p$ < .00001; "Some": ($\beta$ = `r round(rt_lm$coefficients[7], 2)`, $p$ < .004).

This interaction is particularly intriguing because in our previous accuracy model, we found increased performance on these trial types. While we find that older children are taking longer to respond to these trial types, they are more likely to get them correct, even though reaction time is negatively correlated with age. This seems to indicate that succesfully comprehending these difficult scalar terms does require additional processing time. 

##Drift diffusion models
In our previous statistical models, we observed a speed-accuracy tradeoff in older children's performance on "some" and "none" trials. This suggests that children may be taking more time to process these particular quantifiers as they become more familiar with the quantifier scale. A drift diffusion model (DDM) can provide a more detailed view of the relationship between accuracy and reaction time in behavioral tasks [@milosavljevic2010]. \footnote{While DDMs are traditionally used to examine two-alternative forced-choice behavioral decisions, here we use the model to predict the relationship between reaction time and accuracy in making either a correct or incorrect decision in our task.} 

###Parameter estimation
In DDM, a behavioral response (a correct or incorrect choice) is the result of noisy data accumulation (operationalized by response time) [@ratcliff1998]. Responses have *separation boundaries* that are dependent on the amount of information needed to initate a response, and *drift rate* formalizes the rate of data accumulation [@ratcliff1998]. An additional parameter of DDM is *nondecision*, which is the amount of time between encoding the stimuli, and initiating response. Finally, different responses may have a *bias*, or different starting point in the diffusion process, dependent on the stimuli  [@ratcliff1998].  

```{r parameter estimates}
trialtypes = c("All", "Some", "None")

#create dataframe
sub.pars <- data.frame(Separation = numeric(),
                       Non.Decision = numeric(),
                       Bias = numeric(),
                       Drift = numeric(),
                       Trial.Type = character(),
                       SubID = character(), 
                       Age = character())
sub.pars$Trial.Type <- as.character(sub.pars$Trial.Type)
sub.pars$SubID <- as.character(sub.pars$SubID)
sub.pars$Age <- as.character(sub.pars$Age)

temp.pars <- sub.pars

df$resp <- as.character(df$resp)

#this takes a while to run, but estimating parameters for each subject
#Note that this is with excluded reaction time data, but it's possible that we shouldn't be excluding any rts...

# df %<>%
#   dplyr::filter(q > 0, na.rm = TRUE)

subs <- unique(df$sub_id)

for (j in 1:length(subs)) {
  sid <- as.character(subs[j]) 
  for (i in 1:length(trialtypes)) {
    ttype <- as.character(trialtypes[i])
    dat <- as.data.frame(subset(df, trial_type == ttype & sub_id == sid))
    opt <- optim(c(1, .1, .1, 1), wiener_deviance, 
                 dat=dplyr::select(dat, c(q, resp)), method="Nelder-Mead")
    pars <- c(opt$par, ttype, sid, dat$agesplit[1])
    temp.pars[i,] <- pars
  }
  sub.pars <- rbind(temp.pars, sub.pars)
} 
```

```{r parameter df}
sub.pars$Separation <- as.numeric(sub.pars$Separation)
sub.pars$Non.Decision <- as.numeric(sub.pars$Non.Decision)
sub.pars$Bias <- as.numeric(sub.pars$Bias)
sub.pars$Drift <- as.numeric(sub.pars$Drift)

sub.pars.ms <- sub.pars %>%
  gather(Param, Value, Separation:Drift) %>%
  dplyr::group_by(Age, Trial.Type, Param) %>%
  multi_boot_standard("Value", na.rm = TRUE)

sub.pars.ms$Age %<>%
  str_replace("1", "one")%>%
  str_replace("2", "two")%>%
  str_replace("3", "three")%>%
  str_replace("4", "four")%>%
  str_replace("5", "five")%>%
  str_replace("one", "3-3.5") %>%
  str_replace("two", "3.5-4")%>%
  str_replace("three", "4-4.5")%>%
  str_replace("four", "4.5-5")%>%
  str_replace("five", "5-6.5")

# 
```

In fitting a DDM to our data, we estimated parameters for each subject across all three trial types ("all", "some", and "none") using the RWiener package. We then aggregated across subjects to obtain means and confidence intervals for each age group. \footnote{Here we should address reaction time exclusion, when we've made a decision about it.} Figure \ref{fig:param_plot} shows the parameter estimates for each age group, split by trial type. 

```{r param_plot, fig.env = "figure*", fig.pos = "t", fig.width=5.5, fig.height=3.5, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Parameter estimates for drift diffusion model, split by age and trial type. Error bars are 95 percent confidence intervals computed by nonparametric bootstrap."}

quartz()
ggplot(sub.pars.ms, aes(x=Age, y=mean, color=Trial.Type)) +
  geom_point(size = 1.5, position=position_dodge(.2)) + geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position=position_dodge(.2)) +
   facet_wrap(~Param, scales="free") + theme_bw(base_size = 8) + theme(axis.text.x = element_text(angle = 45, hjust = 1,size = 7)) + langcog::scale_color_solarized(guide_legend(title="Trial Type")) + ylab("Mean") + xlab("Age (years)")  

  # theme(axis.text.x = element_text(angle = 45, hjust = 1,size = 7), axis.title.y = element_text(size=8), legend.title=element_text(size = 7), legend.text=element_text(size=6), axis.title.x = element_text(size=8), text = element_text(size = 8))

```


Using the DDM, we can explore in more depth the speed-accuracy tradeoff we observed in both our statistical models. In our separation boundary estimates, we saw that over development the boundary for a correct response decreases, for all trial types, indicating that in general less data needs to be accumulated before making a decision in this task. This is also reflected in the bias estimates, which are very high for all age groups on "all" trials, but increase over development for "some" and "none" trials. Interestingly, we found that non-decision decreases markedly for "all" trials over development, but only slightly for "some" and "none" trials, while drift rates for all quantifiers increase over development. 

In this model, we find that younger children spend less time processing a quantifier, and are also faster in making an incorrect decision with the difficult quantifiers "some" and "none". Young children's high drift rate and comparable nondecision estimates on "all" trials can be attributed to their tendency to select the alternative for "all" on the majority of trials. Later in development, however, we find that older children seem to spend more time processing the quantifiers "some" and "none", and consequently are more likely to make a correct response. Intriguigingly, we observe this trend even though older children are have a greater bias and a lower separation boundary with these quantifier types. 

#General Discussion
Our primary question in this study centered on whether children who succeed on a scalar implicature task (and also understand the quantifiers involved) take more time to process. We adapted a previously validated scalar implicature task [@horowitz2015;@horowitzInPrep] for the iPad to explore whether children's success how response latencies interacted with accuracy in this task. 

In our analyses, we replicated previous results [@horowitz2015; @horowitzInPrep] in our finding that children were overall less accurate when evaluting the quantifiers "some" and "none" in comparison to "all," but that their performance increased over development. We again found evidence of bimodal and correlated performance on these two quantifier types, suggesting a common source of difficulty.

In our extension of this paradigm, we collected reaction time data for these quantifier types to investigate the relationship between reaction time and accuracy. In our reaction time analyses, we found evidence of a speed-accuracy tradeoff, as well as an interaction between reaction time and age, with older children taking a slightly longer time to respond to these trials, but ultimately being more accurate. 

Using a Drift Diffusion Model, we explored reaction time and accuracy patterns in more depth. Overall, the model predicted that children spend more time processing these difficult pragmatic implicatures as they get older, and are more likely to make a correct response as a result. Thus, it appears that success in making a scalar implicature is not only the result of being familiar with the quantifier scale, but also taking additional time to process and integrate in a pragmatic framework in order to make a response.  

Our work contributes to the existing literature in utilizing a novel method to collect accurate and detailed reaction time data on a scalar implicature task. Response latencies are an important indicator of the pragmatic challenges that children face in processing implicatures. Additionally, our findings replicate previous work, providing evidence for the appropriateness of this paradigm in targeting scalar implicatures. Further, our larger sample size, increased number of trials, and randomized design strengthen our analytical power, and allow for more detailed inferences from the data. 

(Final paragraph on quantifier knowledge, inhibitory control, scalar implicatures, and how they relate more broadly to language). 

# Acknowledgements

Special thanks to Bing Nursery School, the San Jose Children's Discovery Museum, Veronica Cristiano, Rachel Walker, and Tamara Mekler for their help with data collection.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
