---
title: "A speed-accuracy tradeoff in children's processing of scalar implicatures"
bibliography: biblibrary.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Rose M. Schneider} \\ \texttt{rschneid@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
    Children's trouble with scalar implicatures -- inferences from a weaker lexicalized description that a stronger alternative is true -- is a puzzle in pragmatic development. Here, we explore reaction time as a measure of processing for scalar implicatures and reasoning about salient alternatives. In our analyses, we explore overall performance and reaction time patterns across development, finding evidence of a speed-accuracy tradeoff for the quantifiers "some" and "none." Motivated by these findings, we use a Drift Diffusion Model to explore the relationship between accuracy and reaction time in processing both scalar implicatures, and the quantifiers "some" and "none" more broadly. Overall, we find evidence that while children's performance in scalar implicature tasks seems to requires additional processing when reasoning about the quantifier scale.
    
keywords:
    Pragmatics; development; language.
    
output: cogsci2016::cogsci_paper
---
```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=TRUE, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(reshape)
# library(plyr)
library(lme4)
library(dplyr)
library(stringr)
library(tidyr)
# library(markdown)
library(directlabels)
library(magrittr)
# library(bootstrap)
library(RCurl)
library(langcog)
# library(RColorBrewer)
library(diptest)
library(RWiener)
# library(ppcor)
theme_set(theme_bw())
```

```{r data}
df <- read.csv("tmp_si.csv")
```

```{r data cleaning}
num_english <- df %>%
  filter(english < 0.75)%>%
  select(sub_id)%>%
  distinct()%>%
  summarise(n = n())

num_subjects <- df %>%
  select(sub_id)%>%
  distinct()

df %<>%
  dplyr::mutate(age = as.numeric(age))%>%
  dplyr::mutate(resp = factor(correct, levels=c("Y","N"), labels=c("upper","lower")), 
         q = rt/1000)%>%
  dplyr::filter(selection_type != "someall" & 
                  selection_type != "allall" & 
                  selection_type != "allallall" & 
                  selection_type != "allsome" & 
                  selection_type != "somesome", na.rm=TRUE) %>% #this is filtering out the strange repeats
  dplyr::filter(english >= 0.75, na.rm=TRUE) %>% # excluding less than 75% english exposure
  dplyr::filter(english != "NA", na.rm=TRUE)%>% #avoiding the missing value that snuck back in
  dplyr::mutate(age_round = round(age, digits = 2))%>% #for easier grouping
  dplyr::mutate(agesplit = cut(age_round, breaks=c(3, 3.5, 4, 4.5, 5, 6.5)),
         agesplit = factor(agesplit,
                           labels=c("3-3.5 years", "3.5-4 years", 
                                    "4-4.5 years", "4.5-5 years", 
                                    "5-6.5 years")))%>% 
  dplyr::filter(agesplit != "NA", na.rm = TRUE) 

#for computing means
df$correct %<>%
  str_replace("Y", 1)%>%
  str_replace("N", 0)

df %<>%
  dplyr::mutate(correct = as.numeric(correct))

#renaming for better graphs at the start
df$trial_type %<>%
  str_replace("all", "All")%>%
  str_replace("some", "Some")%>%
  str_replace("none", "None")
  
df$selection_type %<>%
  str_replace("all", "All")%>%
  str_replace("some", "Some")%>%
  str_replace("none", "None")
```

```{r exclusions}
# Exclusions
df1 <-  subset(df, sub_id != "11716_14" & sub_id != "11316_2" 
               & sub_id != "121815_7" & sub_id != "1616_6" 
               & sub_id != "TM001" & sub_id != "TM003" 
               & sub_id != "TM004" & sub_id != "12416_9" 
               & sub_id!= "TM011") #excluded due to consent, low number of trials, and parental interference

#opaque missing data issue
df <- df1

#for figuring out percentage of trials lost due to RT exclusion
pre_cut <- df
```

```{r rt exclusion}
# qplot(rt, data = df)

#There are some really crazy rts - exclude everything above 15s
df$clean.rt <- df$rt
df$clean.rt[df$rt > 15000] <- NA
mlog <- mean(log(df$clean.rt), na.rm=TRUE)
sdlog <- sd(log(df$clean.rt), na.rm=TRUE)

# qplot(clean.rt, data=df)

# qplot(log(clean.rt), 
#       fill = log(clean.rt) > mlog + 3*sdlog | log(clean.rt) < mlog - 3*sdlog,
#       data = df)

#Assign "NAs" to clean rts 3 sds outside of log of mean rt
df$clean.rt[log(df$clean.rt) > mlog + 3*sdlog | 
              log(df$clean.rt) < mlog - 3*sdlog] <- NA

#filter df to exclude NAs (RT cleaned)
df %<>%
  filter(clean.rt != "NA")

#how much data do we lose in this data cleaning?
total_loss <- as.numeric(nrow(pre_cut) - nrow(df))
percentage <- as.numeric((total_loss/nrow(df))*100)

```
# Introduction
As listeners attempting to comprehend language, we have available to us not only an utterance, but also the knowledge of what the speaker *could* have said. In fact, we frequently go beyond the literal sense of utterances, and use our knowledge about these alternatives to infer a speaker's intended meaning. In the case of *pragmatic implicatures* [@grice1975logic], a speaker employs a weaker literal description to imply that a stronger alternative is true. Thus, an adult listener would strongly infer from the statement ``I enjoyed *some* of my winter break'' that some (but not *all*) of my break was pleasant. This *scalar implicature* (SI) relies heavily on a knowledge of the relevant lexical alternatives in the quantifier scale $<$*none -- some -- all*$>$, as a listener must be able to contrast "some" with the stronger descriptor "all" to compute the implicature. While scalar implicatures are easily comprehended by adults, they pose a pragmatic challenge to children until surprisingly late in development [@horowitz2015;@katsos2011;@papafragou2003]. What is the source of children's difficulties with scalar implicatures?
```{r possible second}
# In contrast to adults' spontaneous processing of such pragmatic implicatures, children display marked and striking failures in such tasks. For example, when judging a scene in which three of three horses have jumped over a fence, preschoolers are likely to endorse the statement "*All* of the horses jumped over the fence" as felicitous, rather than the appropriate statement "*Some* of the horses jumped over the fence" until around five years of age [@papafragou2003]. Interestingly, children do seem to have sensitivity to these scalar terms, and reward speakers based on the informativeness of their scalar descriptions [@katsos2011; @papafragou2004]. Given this early sensitivty, why do children still struggle to compute scalar implicatures until late in development [@noveck2001]? 
```
One possibile cause of children's scalar implicature failures is their knowledge of the relevant lexical alternatives, as proposed by the *Alternatives Hypothesis* [@barner2010; @barner2011]. This hypothesis predicts that if children do not have access to "some," they are unable to directly compare it to "all" in making the scalar implicature *some, but not all.* Due to varying measures and methods in previous scalar implicature research, empirically testing the Alternatives Hypothesis was quite difficult. Children displayed varying performance across these tasks depending on  paradigm, syntactic construction of the implicature prompts, access to visual and lexical alternatives, age, and supportiveness of the task [@guasti2005; @horowitz2015; @noveck2001; @papafragou2003; @papafragou2004]. 

In an attempt to reconcile these various accounts and test the Alternatives Hypothesis, Horowitz and Frank [-@horowitz2015] designed a simple referent selection paradigm that could be used across a broad age range (3--5 years) to explore lexicalized (scalar) implicatures. In this task, children saw three book covers, each featuring four familiar objects (Figure \ref{fig:image}), and the experimenter described a book using a scalar (quantifier) description (e.g., "On the cover of my book, *none/some/all* of the pictures are cats."). Importantly, children had access to lexical alternatives (over the course of the study), as well as visual alternatives (within each trial). 

With this supportive paradigm, Horowitz and Frank found that children struggled with the quantifiers "some" and "none" in relation to "all". Intriguingly, they found that performance between these two quantifiers was strongly bimodal and correlated: Children who failed on trials with the quantifier "some" similarly struggled with "none," and vice versa. While Horowitz & Frank's [-@horowitz2015] findings did provide some support for the Alternatives Hypothesis (with children's scalar implicature performance supported by access to alternatives), they observed that children were not able to capitalize on this knowledge to improve over the course of the study. Therefore, they hypothesized that there was another possible cause for children's poor scalar implicature computation.
```{r image, fig.pos = "b", fig.align='center', fig.width=3, fig.height=3, fig.cap = "Example trial stimuli used in Horowitz and Frank (2015)."}
img <- png::readPNG("figs/implicatures_demo_letters.png")
grid::grid.raster(img)
```
@horowitzInPrep presented two hypotheses for children's observed patterns of performance, namely, lack of quantifier knowledge and developing inhibitory control [@horowitzInPrep]. There, we reasoned that the Alternatives Hypothesis necessitates familiarity with and ability to contrast alternatives on the quantifier scale $<$*none -- some -- all*$>$; if children's quantifier knowledge is absent or unestablished, it might lead to failures in making an implicature. Another possible cause of children's struggles with scalar implicatures might be that they have complete quantifier knowledge, but are unable to inhibit an impulse to choose a more salient alternative (e.g., choosing the book with the most cats upon hearing "On the cover of my book, *some* of the pictures are cats.")  

We explored these two hypotheses in an individual differences task, running our scalar implicature [@horowitz2015] task in conjunction with quantifier-knowledge [@barner2009] and inhibitory control [@zelazo2006] tasks. Overall, we found that children's scalar implicature performance was strongly correlated with quantifier knowledge, even when controlling for age. While inhibitory control was strongly correlated with age, it was not related to children's performance on the scalar implicature task [@horowitzInPrep]. 

Although we found a significant relationship between quantifier knowledge and scalar implicature comprehension, it is likely that children's difficulties in this task has more than one source. Importantly, in the course of conducting this individual differences study, we observed that children who succeeded both in making a scalar implicature and comprehending "none" displayed increased response latencies. This increase in reaction time may be an important indicator of how children are using and evaluting the salient alternatives. 

It is possible that children who have a fully-established quantifier scale must take additional time in using this information to process a scalar implicature. This hypothesis has some support in previous literature; @huang2009 explored online measures scalar implicature processing through eye-tracking, and found a delay in children's location of the referent of a scalar implicature. The exact relationship between reaction time and accuracy using a supportive behavioral paradigm, however, is largely unknown. Here, we explore children's behavioral response latencies in an iPad adaptation of Horowitz and Frank's [-@horowitz2015] scalar implicature task. In this study, we explore our hypothesis that computing a scalar implicature might incur additional processing time for children as they contrast the relevant lexical alternatives to make a correct decision.

In our analyses, we explore overall accuracy and patterns of performance, as in [@horowitz2015; @horowitzInPrep.], and find that children not only struggle in making a scalar implicature, but also grapple with the quantifier "none" until fairly late in development. In examining reaction time patterns across all quantifier types, we find evidence of a speed-accuracy tradeoff associated with these two quantifiers, even later in development. Finally, we use a Drift Diffusion Model to our data to explore the source of this increased reaction time. Overall, our findings indicate that while quantifier knowledge is a key factor in successfully computing scalar implicatures, using this information to successfully compute a scalar implicature is particularly difficult, and seems to requires additional processing.

# Method
In this study, we adapted a scalar mplicature paradigm developed by Horowitz and Frank [-@horowitz2015] for the iPad.\footnote{The full experiment can be viewed online at \texttt{https://rosemschneider.github.io/tablet\_exp/si\_tablet.html}.} In addition to capturing detailed reaction time data, this version included more trials, and standardized prosody across all trials, in addition to a completely randomized design.\footnote{All of our data, processing, experimental stimuli, and analysis code can be viewed in the version control repository for this paper at: \texttt{https://github.com/rosemschneider/SI\_tablet}.} 

## Participants
```{r descriptives}
num_gender <- df%>%
  select(sub_id, sex)%>%
  distinct()%>%
  group_by(sex)%>%
  summarise(n=n())

f <- num_gender %>%
  filter(sex == "F")

m <- num_gender %>%
  filter(sex == "M")

num_trials <- df %>%
  group_by(sub_id, agesplit)%>%
  summarise(n = n())

num_kids <- df %>% 
    dplyr::select(sub_id, agesplit, age) %>%
    dplyr:: distinct()%>%
    dplyr::group_by(agesplit)%>%
    dplyr::summarize(n=n(), mean=mean(age), sd=sd(age), median=median(age))%>%
    dplyr::mutate(total.n = sum(n))

num_young3s <- num_kids %>%
  filter(agesplit == "3-3.5 years")

num_old3s <- num_kids %>%
  filter(agesplit == "3.5-4 years")

num_young4s <- num_kids %>%
  filter(agesplit == "4-4.5 years")

num_old4s <- num_kids %>%
  filter(agesplit == "4.5-5 years")

num_5s <- num_kids %>%
  filter(agesplit == "5-6.5 years")

sstats <- df %>%
    dplyr::group_by(agesplit)%>%
    dplyr::summarise(mean = mean(age), median = median(age), sd = sd(age))

s_young3 <- sstats %>%
  filter(agesplit == "3-3.5 years")

s_old3 <- sstats %>%
  filter(agesplit == "3.5-4 years")

s_young4 <- sstats %>%
  filter(agesplit == "4-4.5 years")

s_old4 <- sstats %>%
  filter(agesplit == "4.5-5 years")

s_5s <- sstats %>%
  filter(agesplit == "5-6.5 years")
```
\begin{table}[ht]
\centering
\begin{tabular}{c c c c c } 
 \hline
 Age group & N & Mean & Median & SD \\
 \hline
 3--3.5 years & `r num_young3s$n` & `r round(s_young3$mean, 2)` & `r round(s_young3$median, 2)` & `r round(s_young3$sd, 2)`\\
 \hline
 3.5--4 years & `r num_old3s$n` & `r round(s_old3$mean, 2)` & `r round(s_old3$median, 2)` & `r round(s_old3$sd, 2)` \\ 
 \hline
 4--4.5 years & `r num_young4s$n` & `r round(s_young4$mean, 2)` & `r round(s_young4$median, 2)` & `r round(s_young4$sd, 2)`\\
 \hline
 4.5--5 years & `r num_old4s$n` & `r round(s_old4$mean, 2)` & `r round(s_old4$median, 2)` & `r round(s_old4$sd, 2)` \\
 \hline
 5--6.5 years & `r num_5s$n` & `r round(s_5s$mean, 2)` & `r round(s_5s$median, 2)` & `r round(s_5s$sd, 2)` \\
 \hline
\end{tabular}
\caption{Age demographic information for all participants.}
\label{tab:age}
\end{table}
Table \ref{tab:age} shows the breakdown of age information for all participants. Included in analyses are `r num_kids$total.n` children out of a planned sample of 120 participants. We ran `r num_english$n` additional children, who were excluded from analysis for low English language exposure ($<75\%$) or $<50\%$ of trials completed. Included in our sample were `r f$n` females and `r m$n` males. ^[Based on @horowitz2015 and @horowitzInPrep, the initially planned sample size was 96 children from 3--5 years. After collecting data from 57 participants, however, we observed significantly lower performance on implicature trials across all age groups, indicating that the iPad adaptation of the scalar implicature task was slightly more challenging for all children, and included an older age group of 24 5--6.5-year-olds.]

## Stimuli
The general format of the task was identical to @horowitz2015, with the exception of added items for additional trials. The study was programmed in HTML, CSS, and JavaScript, and displayed to children on a full-sized iPad. Each trial displayed three book covers, each containing a set of four familiar objects (Figure \ref{fig:image}).  Each session involved 30 trials, with 10 trials per quantifier-type ("all", "some", and "none"). Each audio clip used the same three initial sentence frames (e.g., "On the cover of my book, *some* of the pictures...") so that prodosdy was emphasized equally across all trials. The average length of each audio clip (including target item phrase, e.g., "...are cats") was approximately 6s. In our randomization, quantifier triad order, items (within category), target item, and quantifier were randomized for all participants. In all, there were 270 different target items and audio clips. 

##Procedure
+Sessions took place individually in a small testing room away from the museum floor or the classrooom of the daycare. To familiarize children with the iPad, each session began with a "dot game," which required them to press dots on the screen as fast as possible. After the dot game, the experimenter introduced them to "Hannah," a cartoon character who wanted to play a guessing game with her books. The experimenter explained that Hannah would show the child three books, and would give one hint about which book she had in mind, so they had to listen carefully. Children then saw a practice trial with an unambiguous noun referent. 
  		  
Each trial allowed 2.5s for children to visually inspect the three book covers, before the experiment played the trial prompt (e.g., "On the cover of my book, *none* of the pictures are cats."). Reaction times were measured from the onset of the target word. Children could only make one selection. If a child was not paying attention, or if she did not hear Hannah's prompt, the experimenter repeated it, matching the original prosody. Once children correctly made their selection, a green box appeared around the chosen book. The experiment was self-paced, and children initiated each trial by pressing a button that appeared after they had made their selection in the previous trial.  		  
  		  
```{r overall_acc, fig.pos = "t", fig.width=3.2, fig.height=2, fig.cap = "Children's overall accuracy for each quantifier type. Bars show mean performance for each age group. Error bars are 95 percent confidence intervals computed by non-parametric bootstrap."}
ms <- df %>%
  group_by(trial_type, agesplit)%>%
  multi_boot_standard("correct", na.rm=TRUE)

ms$trial_type %<>%
  str_replace("all", "All")%>%
  str_replace("none", "None")%>%
  str_replace("some", "Some")

quartz()
ggplot(data = ms, 
       aes(x=trial_type, y=mean, fill= agesplit)) +
  geom_bar(stat="identity", position = position_dodge()) +
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .4,
                  show.legend = FALSE,
                 position=position_dodge(.9)) +
  ylab("Proportion correct") + 
  xlab("Trial Type") +
  theme_bw(base_size = 7) + theme(axis.title.x = element_text(size=7),
           axis.title.y  = element_text(size=7)) + 
  scale_fill_solarized(name = "Age") + theme(legend.position=c(.8,.8)) + theme(legend.key.size=unit(.2, "cm")) + theme(legend.title=element_text(size=5)) + theme(legend.text=element_text(size=5))
```

#Results
To exclude trials where the child had missed the prompt or was not paying attention, we excluded reaction times (RTs) longer than 15s. After this initial cut, we excluded RTs outside three standard deviations of the log of mean reaction time. This cleaning process resulted in a data loss of `r total_loss` trials (`r round(percentage, 2)`%). 

##Accuracy
```{r t tests}
t_tests <- aggregate(correct ~ trial_type + agesplit +  sub_id, data=df, mean)

#tests against quantifiers

#3-3.5s all and none
young3_allnone <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="All")$correct)
#all and some
young3_allsome <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="All")$correct)
#some and none
young3_somenone <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="Some")$correct)

#3.5s-4s 
old3_allnone <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="All")$correct)
old3_allsome <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="All")$correct)
old3_somenone <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="Some")$correct)

#4-4.5s
young4_allnone <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="All")$correct)
young4_allsome <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="All")$correct)
young4_somenone <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="Some")$correct)

#4.5-5s
old4_allnone <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="All")$correct)
old4_allsome <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="All")$correct)
old4_somenone <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="Some")$correct)

#5s
fives_allnone <- t.test(subset(t_tests, agesplit=="5-6.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="5-6.5 years" & trial_type=="All")$correct)
fives_allsome <- t.test(subset(t_tests, agesplit=="5-6.5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="5-6.5 years" & trial_type=="All")$correct)
fives_somenone <- t.test(subset(t_tests, agesplit=="5-6.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="5-6.5 years" & trial_type=="Some")$correct)

```
###Overall accuracy
Figure \ref{fig:overall_acc} shows children's for each trial type, split by age group. For each age group, we saw significantly lower accuracy for the quantifiers "some" and "none" in comparison to "all" (all $p$s $< .01$ in two-sample t-tests for each age group). These results generally replicate our previous findings using this paradigm [@horowitz2015], but one difference from the previous results was in implicature trials. Children aged 3--5 years performed significantly lower on "some" (implicature) trials in this task in comparison with data from @horowitzInPrep ($p < .01$ for all tests). Thus, while the iPad adaptation was generally successful, implicatures were more difficult, perhaps because of the non-social nature of the iPad interaction or the recorded audio stimuli. 
```{r comparison}
d_si <- read.csv("experiment3.csv") #pull in aggregated data from previous analysis

t_tests %<>%
  spread(trial_type, correct)
#only for some and none trials, because we don't find that the all trials are much affected by the transfer to the iPad
#3-3.5 years
list <- c(
t.test(subset(t_tests, agesplit=="3-3.5 years")$None, subset(d_si, Age=="3-3.5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="3-3.5 years")$Some, subset(d_si, Age=="3-3.5 years")$Some, var.equal = TRUE),

#3.5-4s
t.test(subset(t_tests, agesplit=="3.5-4 years")$None, subset(d_si, Age=="3.5-4 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="3.5-4 years")$Some, subset(d_si, Age=="3.5-4 years")$Some, var.equal = TRUE),

#4-4.5s
t.test(subset(t_tests, agesplit=="4-4.5 years")$None, subset(d_si, Age=="4-4.5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="4-4.5 years")$Some, subset(d_si, Age=="4-4.5 years")$Some, var.equal = TRUE),

#4.5-5s
t.test(subset(t_tests, agesplit=="4.5-5 years")$None, subset(d_si, Age=="4.5-5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="4.5-5 years")$Some, subset(d_si, Age=="4.5-5 years")$Some, var.equal = TRUE))
```

```{r diptest, fig.pos = "t", fig.width=3.2, fig.height=2, fig.cap = "Frequency histogram of correct responses for each trial type, across all participants."}
ms <- df %>%
  group_by(sub_id, trial_type) %>%
  multi_boot_standard("correct", na.rm=TRUE)

diptest_all <- diptest::dip.test(filter(ms, trial_type == "All")$mean)
diptest_some <- diptest::dip.test(filter(ms, trial_type == "Some")$mean)
diptest_none <- diptest::dip.test(filter(ms, trial_type == "None")$mean)

ms <- df %>%
  group_by(sub_id, trial_type) %>%
  summarise(correct = sum(correct))
  
quartz()
ggplot(ms, aes(x = correct, fill=trial_type)) + 
  geom_histogram(binwidth = 1) + 
  theme_bw(base_size = 7) + 
  xlab("Total correct responses per participant") + 
  ylab("Frequency") + 
  facet_wrap(~trial_type) + 
  theme(legend.position="none") + 
  theme(axis.title.x = element_text(size = 7), axis.title.y = element_text(size = 7)) +
  scale_x_continuous(breaks = c(0,2,4,6,8,10)) + 
  langcog::scale_fill_solarized()
```

```{r dense, fig.env = "figure*", fig.pos = "t", fig.width=5.75, fig.height=3, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Density plots of reaction times for correct and incorrect responses on each trial type, split by age."}
trialtypes <- c("all", "some", "none")

#For easier to read graph
df$resp1 <- df$resp %>%
  str_replace("lower", "Incorrect")%>%
  str_replace("upper", "Correct")

#Stats for median lines
rt_stats <- df %>%
  group_by(agesplit, trial_type, resp1)%>%
  summarise(mean=mean(rt), median = median(rt))%>%
  mutate(m = median/1000)

m_upper <- rt_stats %>%
  filter(resp1 == "Correct")

m_lower <- rt_stats %>%
  filter(resp1 == "Incorrect")

#for easier to read graph
df$resp1 <- factor(df$resp1, levels=c("Incorrect", "Correct"))

# quartz()
ggplot(df, aes(x=q)) + 
  geom_density(aes(weight = sum(resp1=="Correct")/length(resp1), fill = resp1), alpha=0.3) +
  facet_grid(agesplit ~ trial_type) + 
#   geom_vline(data = m_upper, aes(xintercept = m), col = "blue", lty = 2) + 
#   geom_vline(data = m_lower, aes(xintercept = m), col = "red", lty = 2) + 
  scale_fill_solarized(name = "Response") +
  xlab("Response time (s)") + 
  ylab("Density of responses") + 
  xlim(c(0,10)) + 
  theme_bw(base_size = 7) + 
  theme(axis.title.x = element_text(size=7),
        axis.title.y  = element_text(size=7),
        legend.key.size=unit(.3, "cm"),
        legend.title=element_text(size=5.5),
        legend.text=element_text(size=5)) 
```

```{r accuracy model}
lm <- summary(glmer(correct ~ age * trial_type + 
               (trial_type | sub_id), 
             family = "binomial", data = df))
```
We next fit a logistic mixed effects model predicting correct response as an interaction of age and trial type, with random effects of trial type and participant.\footnote{All mixed effects models were fit in \texttt{R} using the \texttt{lme4} package. The model specification was: \texttt{correct ~ age * trial type + (trial type | subject id)}.} Performance was significantly lower on "some" ($\beta = `r round(lm$coefficients[4], 2)`$, $p < .0001$) and "none" trials ($\beta = `r round(lm$coefficients[3], 2)`$, $p < .0001$). There was also a signficant interaction between age and trial type on "none" trials ($\beta$ = `r round(lm$coefficients[5], 2)`, $p$ < .0001), indicating that children's performance with this difficult quantifier increased with age. 

```{r correlation}
ms.acc <- df %>%
  dplyr::group_by(trial_type, agesplit, sub_id) %>%
  multi_boot_standard("correct", na.rm = TRUE) %>%
  dplyr::select(-ci_lower, -ci_upper)%>%
  spread(trial_type, mean)

# ms.acc.p <- df %>%
#   dplyr::group_by(trial_type, age, sub_id) %>%
#   multi_boot_standard("correct", na.rm = TRUE) %>%
#   dplyr::select(-ci_lower, -ci_upper)%>%
#   spread(trial_type, mean)%>%
#   dplyr::filter(Some != "NA" & None != "NA")%>%
#   dplyr::mutate(age = as.numeric(age))

#ggcorplot(ms.acc %>% filter(complete.cases(ms.acc)) %>% dplyr::select(None, Some, All))

#correlation test
sn_cor <- cor.test(ms.acc$Some, ms.acc$None)

r_corr <- round(sn_cor$estimate, digits = 2)
p_corr <- round(sn_cor$p.value, digits = 6)
# sn_pcor <- pcor.test(ms.acc.p$Some, ms.acc.p$None, ms.acc.p$age)
```

```{r trials correct}
#How many children got the majority of trials correct? 
perc <- df %>%
  group_by(sub_id, trial_type, correct)%>%
  summarise(n=n())%>%
  group_by(trial_type)%>%
  filter(correct == 1, trial_type == "Some" | trial_type == "None")%>%
  filter(n >= 9)

none_perc <- perc %>%
  filter(trial_type == "None")

some_perc <- perc %>%
  filter(trial_type == "Some")
```
Figure \ref{fig:diptest} shows distributions of correct responses for all trial types. Performance on "some" and "none" trials was bimodal (Hartigan's $D$ = `r round(diptest_some$statistic, 2)`, $p$ < .0001) and "none" trials ($D$ = `r round(diptest_none$statistic, 2)`, $p$ < .0001). While children's average accuracy was low for these quantifiers, there were some children who were correct on the majority of these trials ("Some": N = `r nrow(some_perc)`; "None": N = `r nrow(none_perc)`) and the others were typically incorrect on the majority of trials. Children did not appear to be responding randomly. As in previous work, we found a strong correlation between children's accuracy on "some" and "none" trials ($r$ = `r r_corr`, $p$ < .0001). 

##Reaction time 
```{r rt summary stats}
rt_sstats <- df %>%
  gather(measure, rt, rt, clean.rt) %>%
  group_by(trial_type, measure) %>%
  summarise(mean = mean(rt, na.rm=TRUE), 
            sd = sd(rt, na.rm=TRUE), 
            max = max(rt, na.rm=TRUE), 
            min = min(rt, na.rm=TRUE), 
            median=median(rt, na.rm=TRUE)) 
```

```{r rt corr}
#correlation between reaction time and age
age_corr <- cor.test(df$age, df$rt)
```

```{r rt corr_1}
ms.rt <- df %>%
  dplyr::group_by(trial_type, agesplit, sub_id) %>%
  dplyr::summarise(rt = mean(clean.rt, na.rm=TRUE)) %>%
  spread(trial_type, rt) 
# 
# ggcorplot(ms.rt %>% filter(complete.cases(ms.rt)) %>% dplyr::select(None, Some, All))

allsome_corr <- cor.test(ms.rt$All, ms.rt$Some)
allnone_corr <- cor.test(ms.rt$All, ms.rt$None)
somenone_corr <- cor.test(ms.rt$Some, ms.rt$None)
```

We fit a linear mixed effects model predicting log RT on correct trials as a function of log trial number, the interaction of age and trial type, and random effects of trial type by subject.^[Model specification: \texttt{log(reaction time) ~ log(trial number) + age * trial type + (trial type | subject id)}. Age was centered for ease of interpretation of coefficients, and we calculated \emph{p} values via the $t=z$ approximation.] Reaction times were longer on "none" ($\beta = `r round(rt_lm$coefficients[4], 2)`$, $p < .0001$) and "some" trials ($\beta = `r round(rt_lm$coefficients[5], 2)`$, $p < .0001$), and reaction times decreased with age ($\beta = `r round(rt_lm$coefficients[3], 2)`$, $p < .0001$). There were no significant interactions between age and trial type. The model also showed a main effect of trial number, with reaction times decreasing over the course of the study ($\beta$ = `r round(rt_lm$coefficients[2], 2)`, $p$ < .00001). 

```{rt_acc model}
rt_acc_lm <- summary(lmer(log(clean.rt) ~ log(trial_num) + 
    scale(age, scale=FALSE) * trial_type * correct + 
    (trial_type | sub_id), 
    data = df))
```

Examination of the pattern in Figure \ref{fig:dense} suggests that accuracy and reaction time may be interacting, however. In particular, while correct responses on "all" trials appear to be faster than the (few) incorrect responses, the opposite is true for "none" and "some" trials: Errors have faster RTs, potentially indicating a speed-accuracy tradeoff. To test for this effect, we fit another mixed effects model, this time including accuracy and its interactions with age and trial type as predictors. This model revealed that correcvt trials overall had faster RTs ($\beta = `r round(rt_acc_lm$coefficients[6], 2)`$, $p = .0002$), but that this accuracy term interacted negatively with trial type such that both "none" and "some" trials had slower RTs for correct trials ($\beta = `r round(rt_acc_lm$coefficients[10], 2)`$, $p < .0001$; $\beta = `r round(rt_acc_lm$coefficients[11], 2)`$, $p < .0001$). There were no three-way interactions of trial-type and age. This model thus provides evidence of a speed-accuracy tradeoff for "some" and "none" trials. 
```{r parameter estimates}
# ms.acc <- df %>%
#   group_by(sub_id, trial_type) %>%
#   multi_boot_standard("correct", na.rm=TRUE)%>%
#   select(-ci_lower, -ci_upper)%>%
#   spread(trial_type, mean)%>%
#   filter(Some >= .75)
# 
# tmp <- df %>%
#   filter(accuracy == "high")%>%
#   group_by(sub_id)%>%
#   distinct()


# for(i in df)
#   df$accuracy <- NA
#   
# if(df$sub_id == "11016_4" || df$sub_id == "11316_11" || df$sub_id == "11316_4"
#          || df$sub_id == "11316_8" || df$sub_id == "11716_1" || df$sub_id == "11716_12"
#          || df$sub_id == "11716_2" || df$sub_id == "11716_7" || df$sub_id == "12016_3" 
#          || df$sub_id == "12016_7" || df$sub_id == "12116_1" || df$sub_id == "12116_3" 
#          || df$sub_id == "121815_3" || df$sub_id == "12216_5" 
#          || df$sub_id == "12416_2" || df$sub_id == "12416_6" || df$sub_id == "12816_1"
#          || df$sub_id == "12816_2" || df$sub_id == "12816_3" || df$sub_id == "1616_2" 
#          || df$sub_id == "1616_5"
#          ||df$sub_id == "Rs5" || df$sub_id == "Rs8" || df$sub_id == "TM002" || df$sub_id == "TM005" || df$sub_id == "TM009") {
#     df$accuracy <- "high"
# } else {
#            df$accuracy <- "low"
#          }

# param.high <-subset(df, sub_id == "11016_4" | sub_id == "11316_11" | sub_id == "11316_4"
#          | sub_id == "11316_8" | sub_id == "11716_1" | sub_id == "11716_12"
#          | sub_id == "11716_2" | sub_id == "11716_7" | sub_id == "12016_3" | sub_id == "12016_7"
#          | sub_id == "12116_1" | sub_id == "12116_3" | sub_id == "121815_3" | sub_id == "12216_5" 
#          | sub_id == "12416_2" | sub_id == "12416_6" | sub_id == "12816_1"
#          | sub_id == "12816_2" | sub_id == "12816_3" | sub_id == "1616_2" | sub_id == "1616_5"
#          | sub_id == "Rs5" | sub_id == "Rs8" | sub_id == "TM002" | sub_id == "TM005" | sub_id == "TM009")

trialtypes = c("All", "Some", "None")

# #create dataframe for high accuracy
# sub.pars <- data.frame(Separation = numeric(),
#                        Non.Decision = numeric(),
#                        Bias = numeric(),
#                        Drift = numeric(),
#                        Trial.Type = character(),
#                        SubID = character(), 
#                        Age = character())
# sub.pars$Trial.Type <- as.character(sub.pars$Trial.Type)
# sub.pars$SubID <- as.character(sub.pars$SubID)
# sub.pars$Age <- as.character(sub.pars$Age)
# 
# temp.pars <- sub.pars


sub.pars.high <- data.frame(Separation = numeric(),
                       Non.Decision = numeric(),
                       Bias = numeric(),
                       Drift = numeric(),
                       Trial.Type = character(),
                       SubID = character(), 
                       Age = character())
sub.pars.high$Trial.Type <- as.character(sub.pars.high$Trial.Type)
sub.pars.high$SubID <- as.character(sub.pars.high$SubID)
sub.pars.high$Age <- as.character(sub.pars.high$Age)

temp.pars.high <- sub.pars.high

# now for low
sub.pars.low <- data.frame(Separation = numeric(),
                       Non.Decision = numeric(),
                       Bias = numeric(),
                       Drift = numeric(),
                       Trial.Type = character(),
                       SubID = character(), 
                       Age = character())
sub.pars.low$Trial.Type <- as.character(sub.pars.low$Trial.Type)
sub.pars.low$SubID <- as.character(sub.pars.low$SubID)
sub.pars.low$Age <- as.character(sub.pars.low$Age)

temp.pars.low <- sub.pars.low

df$resp <- as.character(df$resp)

#this takes a while to run, but estimating parameters for each subject
#Note that this is with excluded reaction time data, but it's possible that we shouldn't be excluding any rts...

# df %<>%
#   dplyr::filter(q > 0, na.rm = TRUE)

param.high <- df %>%
  filter(accuracy == "high")

param.low <- df %>%
  filter(accuracy == "low")

subs.high <- unique(param.high$sub_id)
subs.low <- unique(param.low$sub_id)

# subs <- unique(df$sub_id)

# for (j in 1:length(subs)) {
#   sid <- as.character(subs[j]) 
#   for (i in 1:length(trialtypes)) {
#     ttype <- as.character(trialtypes[i])
#     dat <- as.data.frame(subset(df, trial_type == ttype & sub_id == sid))
#     opt <- optim(c(1, .1, .1, 1), wiener_deviance, 
#                  dat=dplyr::select(dat, c(q, resp)), method="Nelder-Mead")
#     pars <- c(opt$par, ttype, sid, dat$agesplit[1])
#     temp.pars[i,] <- pars
#   }
#   sub.pars <- rbind(temp.pars, sub.pars)
# } 


# first estimate params for high accuracy kids
for (j in 1:length(subs.high)) {
  sid <- as.character(subs.high[j]) 
  for (i in 1:length(trialtypes)) {
    ttype <- as.character(trialtypes[i])
    dat <- as.data.frame(subset(df, trial_type == ttype & sub_id == sid))
    opt <- optim(c(1, .1, .1, 1), wiener_deviance, 
                 dat=dplyr::select(dat, c(q, resp)), method="Nelder-Mead")
    pars <- c(opt$par, ttype, sid, dat$agesplit[1])
    temp.pars.high[i,] <- pars
  }
  sub.pars.high <- rbind(temp.pars.high, sub.pars.high)
} 
# 
# now for low
for (j in 1:length(subs.low)) {
  sid <- as.character(subs.low[j]) 
  for (i in 1:length(trialtypes)) {
    ttype <- as.character(trialtypes[i])
    dat <- as.data.frame(subset(param.low, trial_type == ttype & sub_id == sid))
    opt <- optim(c(1, .1, .1, 1), wiener_deviance, 
                 dat=dplyr::select(dat, c(q, resp)), method="Nelder-Mead")
    pars <- c(opt$par, ttype, sid, dat$agesplit[1])
    temp.pars.low[i,] <- pars
  }
  sub.pars.low <- rbind(temp.pars.low, sub.pars.low)
} 
```

```{r parameter df}
#bind two param estimates together for high and low
sub.pars <- rbind(sub.pars.low, sub.pars.high)
# sub.pars <- sub.pars.high

#high
sub.pars$Separation <- as.numeric(sub.pars$Separation)
sub.pars$Non.Decision <- as.numeric(sub.pars$Non.Decision)
sub.pars$Bias <- as.numeric(sub.pars$Bias)
sub.pars$Drift <- as.numeric(sub.pars$Drift)

sub.pars <- select(sub.pars, -Age) %>%
  rename(sub_id = SubID) %>%
  left_join(df %>% 
              group_by(sub_id) %>% 
              distinct %>% 
              select(sub_id, age_round, accuracy)) %>%
  gather(Param, Value, Separation:Drift) 


sub.pars.ms <- sub.pars %>%
  group_by(accuracy) %>%
  mutate(Age = mean(age_round)) %>%
  group_by(accuracy, Trial.Type, Param) %>%
  multi_boot_standard("Value", na.rm = TRUE)
```

```{r param_plot, fig.env = "figure*", fig.pos = "t", fig.width=6.6, fig.height=1.45, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Parameter estimates for drift diffusion model, split by age and trial type. Error bars are 95 percent confidence intervals computed by nonparametric bootstrap."}

quartz()
ggplot(sub.pars.ms, aes(x=accuracy, y=mean, color=Trial.Type)) +
  geom_point(size = 1.2, position=position_dodge(.05)) + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                 position=position_dodge(.05)) +
  geom_line(aes(colour = Trial.Type, group=Trial.Type)) + 
  facet_wrap(~Param, scales="free", ncol = 4) + 
  theme_bw(base_size = 7.7) + 
  theme(axis.text.x = element_text(size = 7),
        axis.title.x = element_text(size = 7.5),
        axis.title.y = element_text(size = 7.5),
        axis.text.y = element_text(size = 7),
        legend.text = element_text(size = 6), 
        legend.title = element_text(size = 6.5)) + 
  langcog::scale_color_solarized(guide_legend(title="Trial Type")) + 
  ylab("Mean") + xlab("Accuracy")
```

###Statistical modeling
```{r rt model}
rt_lm <- summary(lmer(log(rt) ~ scale(age, scale=FALSE) * log(trial_num) + 
               scale(age, scale=FALSE) * trial_type + 
               (trial_type | sub_id), 
             data = df))
```

We next turned to the relationship betwen age, reaction time, and accuracy. Our initial hypothesis was that successfully computing quantifiers might require additional processing time to contrast salient alternatives. In exploring this, we ran a planned linear mixed effects model predicting the log of reaction time as an interaction of age, trial number, and trial type with a random effect of trial type.^[Mixed effects model fit in R using the lme4 package. The model specifications were as follows: \texttt{log(reaction time) ~ scale(age) * log(trial number) + scale(age) * trial type + (trial type | subject id)}. We calculated \emph{p} values by treating the \emph{t} statistic as if it were a \emph{z} statistic @barr2013.] We found a main effect of trial number, with reaction times decreasing over the course of the study (($\beta$ = `r round(rt_lm$coefficients[3], 2)`, $p$ < .00001)), but found longer reaction times on "none" ($\beta$ = `r round(rt_lm$coefficients[4], 2)`, $p$ < .00001), and "some" trials ($\beta$ = `r round(rt_lm$coefficients[5], 2)`, $p$ < .00001). Interestingly, we also found an interaction between age and trial type, such that reaction times on "none" trials increased with age relative to "all" trials ($\beta$ = `r round(rt_lm$coefficients[6], 2)`, $p$ < .02), and marginally (but not significantly) increased on "some" trials ($\beta$ = `r round(rt_lm$coefficients[7], 2)`, $p$ = .38). 

This interaction is particularly intriguing because in our previous accuracy model, we found increased performance on these trial types. While we find that older children are taking longer to respond to these trial types, they are more likely to answer correctly, even though reaction time is negatively correlated with age. This seems to indicate that there is a speed-accuracy tradeoff for these quantifiers.

```{r}
sep_lm <- summary(lmer(Value ~ accuracy * Trial.Type + (1 | sub_id), 
             data = filter(sub.pars, Param=="Separation")))
sep_lmage <- summary(lmer(Value ~ accuracy* Trial.Type *age_round + (1 | sub_id), 
             data = filter(sub.pars, Param=="Separation")))

nd_lm <- summary(lmer(Value ~ accuracy * Trial.Type + (1 | sub_id), 
             data = filter(sub.pars, Param=="Non.Decision")))
nd_lm_age <- summary(lmer(Value ~ accuracy * Trial.Type *age_round + (1 | sub_id), 
             data = filter(sub.pars, Param=="Non.Decision")))

bias_lm <- summary(lmer(Value ~ accuracy* Trial.Type + (1 | sub_id), 
             data = filter(sub.pars, Param=="Bias")))
bias_lmage <- summary(lmer(Value ~ accuracy * Trial.Type * age_round + (1 | sub_id), 
             data = filter(sub.pars, Param=="Bias")))

drift_lm <- summary(lmer(Value ~ accuracy* Trial.Type + (1 | sub_id), 
             data = filter(sub.pars, Param=="Drift")))
drift_lmage <-summary(lmer(Value ~ accuracy* Trial.Type *age_round + (1 | sub_id), 
             data = filter(sub.pars, Param=="Drift")))
```

##Drift diffusion models
Our preliminary analysis of children's reaction times in this task indicated greater response latencies associated with success on "some" and "none" trials. This suggests that children may be taking more time to actively compare and contrast scalar alternatives as they become more familiar with the quantifier scale. In an exploratory analysis, we fit a drift diffusion model (DDM) to our data. A DDM can be used in behavioral tasks to provide a more detailed view of the relationship between accuracy and reaction time [@milosavljevic2010; @ratcliff1998].

###Parameter estimation
In DDM, a behavioral response (a correct or incorrect choice) is the result of noisy data accumulation through a diffusion process (operationalized by response time) [@ratcliff1998]. Responses have *separation boundaries* that are dependent on the amount of information needed to initate a response, and *drift rate* formalizes the rate of data accumulation [@ratcliff1998]. An additional parameter of DDM is *nondecision*, which is the amount of time between stimuli offset, and initiating response. Finally, different responses may have a *bias*, or different starting point in the diffusion process, dependent on the stimuli [@ratcliff1998]. 

Although a DDM is traditionally used in two-alternative forced-choice tasks, here we are estimating the drift process between a correct and incorrect choice, with two options in each trial being "incorrect," and only one being consistent with the target noun and quantifier. In fitting a DDM to our data in this way, we split children by accuracy on scalar implicature trials (high or low), and then estimated parameters for each subject across all three trial types independently by accuracy group. High accuracy was defined as an average of 75\% performance on scalar implicature trials. Our goal in this exploratory analysis was to explore whether the drift process shows differences for children who succeed in this task versus children who fail, thus the need to estimate parameters for high and low accuracy groups separately. We estimate paramters using the RWiener package, and then aggregated across subjects to obtain means and confidence intervals for each accuracy group. Figure \ref{fig:param_plot} shows the paramter estimates for each accuracy group, split by trial type.

In our separation boundary parameter estimates, we did not find a significant difference of trial types for either accuracy group. This indicates that roughly the same amount of data must be accumulated for each trial to make a response. In nondecision estimates, "none" seems to have a higher nondecision time overall, which we found with a mixed effects model ($\beta$ = `r round(nd_lm$coefficients[3],2)`, $p$ = .006).\footnote{The specifications for all parameter models are as follows (age coefficient added when specified): \texttt{Parameter Value ~ age * trial type (* age) + (1 | subject ID)}}. 

While drift rates show a significant effect of accuracy, because we estimated parameters for high and low accuracy children separately, these are defined by the analysis. In our bias estimates, however, we found a significant interaction between accuracy group and trial type on "some" trials ($\beta$ = `r round(bias_lm$coefficients[6], 2)`, $p$ = .0013). This interaction suggests that bias (the starting point in the diffusion process) might be an important factor in successfully making a scalar implicature. As another exploratory analysis, however, we included an age coefficient in this model, and found that this interaction was no longer significant; instead, we found a significant interaction between age, trial type "some", and accuracy ($/beta$ = `r round(bias_lmage$coefficients[12], 2)`, $p$ = .024). While it seems that when accounting for age children's bias on "some" trials is not affected by scalar implicature accuracy, older children in the low accuracy group are significantly more likely to have a lower bias for "some." 
```{r old param estimates}
# In fitting a DDM to our data in this way, we estimated parameters for each subject across all three trial types ("all", "some", and "none") using the RWiener package. We then aggregated across subjects to obtain means and confidence intervals for each age group. Figure \ref{fig:param_plot} shows the parameter estimates for each age group, split by trial type. 
```

```{r old DDM exp}
# Using the DDM, we can explore in more depth the speed-accuracy tradeoff we observed in our statistical models. In our separation boundary estimates, we did not see any significant effect of trial type on boundary estimates, indicating that roughly the same amount of information needs to be accumulated to make a decision in each trial type. We also did not find an effect of trial type on drift rate, although in a mixed effects model we did find a main of effect of age on drift rate ($\beta$ `r round(drift_lm$coefficients[2], 2)`, $p$ = .01).\footnote{The specifications for all parameter models are as follows: \texttt{Parameter Value ~ age * trial type + (1 | subject ID)}} We did not find that children accumulated data significantly faster for "some" and "none" trials.
# 
# In our nondecision parameter estimates, we again did not find a significant effect of quantifier type, although in a mixed effects model we found a main effect of age ($\beta$ = `r round(nd_lm$coefficients[2], 2)`, $p$ < .00001), and a significant interaction between age and "none" trials ($\beta$ = `r round(nd_lm$coefficients[5], 2)`, $p$ = .01). Thus, the model predicts that although older children have lower nondecision times, these times are higher for older children on "none" trials (in comparison to "all") trials.
# 
# Finally, in our bias estimates, we found high bias across all age groups on "all" trials, and markedly lower estimates for "some" trials. Bias estimates for "none" trials increases over development, and we found a significant interaction between age and trial type on "none" trials ($\beta$ = `r round(bias_lm$coefficients[5],2)`, $p$ = .023). This higher bias for "none" versus "some" may be a result of the additional step that children must make in computing this quantifier. Additionally, previous research indicates that when making a scalar implicature, "none" is a particularly salient and compelling alternative [@franke2014].
# 
# Taken together, the parameter estimates from our DDM indicate that as children develop (and become more familiar with the quantifier scale), they are more likely to respond correctly in our scalar implicature task (especially on "none"). Younger children are more likely to respond incorrectly, mostly due to a low rate of data accumulation and a high separation boundary. Children's bias to respond correctly for all trial types seems to increase with age, and nondecision appears to decrease. While these findings do not fully explain the interaction between response time and accuracy we observed in our behavioral data, they do indicate some developmental differences in processing underying children's development in computing implicatures.
```

#General Discussion
Our primary question in this study centered on whether success in making scalar implicatures requires increased response latencies to make use of relevant scalar alternatives. We adapted a previously validated scalar implicature task [@horowitz2015;@horowitzInPrep] for the iPad to explore the relationship between reaction time and accuracy.

In our analyses, we replicated previous patterns of performance in @horowitz2015 and @horowitzInPrep. We found that children were overall less accurate when evaluting the quantifiers "some" and "none" in comparison to "all," but that their performance increased over development. We again found evidence of bimodal and correlated performance on these two quantifier types, suggesting a common source of difficulty. Additionally, we discovered in a statistical model that although children were more likely make an incorrect response on "some" and "none" trial, their performance on these trials signficantly increased with age.

In our extension of this paradigm, we collected reaction time data for these quantifier types to investigate the relationship between reaction time and accuracy. In our reaction time analyses, we found evidence of a speed-accuracy tradeoff, as well as an interaction between reaction time and age, with older children taking a slightly longer time to respond to these trials, but ultimately being more accurate. These findings motivated our decision to fit our data to a Drift Diffusion Model. 

As an exploratory analysis, we fit a DDM to our data to prediction the relationship between reaction time and accuracy for children who succeed in making scalar implicatures versus children who fail. While the results are preliminary, we found some evidence that bias might be a critical factor in success in making a scalar implicature. Interestingly, this effect seems to be driven by age, such that older children who fail on these trials have significantly lower bias in comparison to "all" trials. It is very likely that the high variability and wide distribution of reaction times observed in this study contributed to the unclear findings observed in our DDM. Given that we did see indications that bias is a significant part of successfully making a scalar implicature, however, future work should explore this relationship.
```{r old DDM conclusion}
# Using a DDM, we explored reaction time and accuracy patterns in more depth. Overall, the model predicted that young children accumulate data quickly, but are more likely to make an incorrect response on "some" and "none" trials. With age, children do not accumulate data more quickly, but are more biased to make a correct response on these trials. Thus, it appears that success in making a scalar implicature is not only the result of being familiar with the quantifier scale, but also taking additional time to process and integrate in a pragmatic framework in order to make a response.
```
Our work contributes to the existing literature in utilizing a novel method to collect accurate and detailed reaction time data on a scalar implicature task. Response latencies are an important indicator of the pragmatic challenges that children face in processing implicatures. Additionally, our findings replicate previous work, providing evidence for the appropriateness of this paradigm in targeting scalar implicatures. Further, our larger sample size, increased number of trials, and randomized design strengthen our analytical power, and allow for more detailed inferences from the data. Our work supports not only the hypthesis that children must be familiar with the quantifier scale in order to make an implicature, but also provides preliminary evidence that doing correctly doing so may require additional processing time. 

Taken together, our work suggests that there is a meaningful relationship between children's accuracy and reaction times in making scalar implicatures throughout development. From our results, the relationship between children's quantifier knowledge, processing speed, and scalar implicature computation is unclear, and future work should test these links more explicitly. Our work suggests, however, that comparing a speaker's statement to possible alternatives in order to make an implicature is an active process for children, and does seem to result in a speed-accuracy tradeoff.  

# Acknowledgements

Thanks to Bing Nursery School and the San Jose Children's Discovery Museum. Thanks also to Veronica Cristiano, Rachel Walker, and Tamara Mekler for their help with data collection, and to Kara Weisman and Ann Nordmeyer for their assistance creating stimuli.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
