---
title: "A speed-accuracy trade-off in children's processing of scalar implicatures"
bibliography: biblibrary.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information:
    \author{{\large \bf Rose M. Schneider} \\ \texttt{rschneid@stanford.edu} \\ Department of Psychology \\ Stanford University \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
    Scalar implicatures---inferences from a weak description ("I ate some of the cookies") that a stronger alternative is true ("I didn't eat all")---are paradigm cases of pragmatic inference. Children's trouble with scalar implicatures is thus an important puzzle for theories of pragmatic development, given their communicative competence in other domains. Previous research has suggested that access to alternatives might be key. Here, we explore children's reaction times in a new paradigm for measuring scalar implicature processing. Alongside failures on scalar implicatures with "some," we replicate previous reports of failures with "none," and find evidence of a speed-accuracy trade-off for both quantifiers. Motivated by these findings, we explore the relationship between accuracy and reaction time with a Drift Diffusion Model. We find evidence consistent with the hypothesis that preschoolers lack access to the alternatives for scalar implicature computation, although this set of alternatives may be broader than previously assumed.
  
keywords:
    Pragmatics; development; scalar implicature; diffusion models.
  
output: cogsci2016::cogsci_paper
---
```{r global_options, include=FALSE}
#knitr setup
#cache chunks to run more quickly; if you make any changes to the df however, reknit with cache = FALSE to incorporate those changes
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
        echo=F, warning=F, cache=TRUE, message=F, sanitize = T)
```

```{r, libraries}
#load necessary libraries
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(reshape)
library(lme4)
library(dplyr)
library(stringr)
library(tidyr)
library(directlabels)
library(magrittr)
library(RCurl)
library(langcog)
library(diptest)
library(RWiener)

theme_set(theme_bw())
```

```{r data}
#load main dataframe
df <- read.csv("tmp_si.csv")
```

```{r data cleaning}
#for demographics - number of children with < 75% English exposure
num_english <- df %>%
  filter(english <= 0.75)%>%
  select(sub_id)%>%
  distinct()%>%
  summarise(n = n())

#overall number of subjects
num_subjects <- df %>%
  select(sub_id)%>%
  distinct()

#cleaning up the main df
df %<>%
  dplyr::mutate(age = as.numeric(age))%>%
  dplyr::mutate(resp = factor(correct, levels=c("Y","N"), labels=c("upper","lower")), 
   q = rt/1000)%>% #factoring responses for RT density analyses
  dplyr::filter(selection_type != "someall" & 
      selection_type != "allall" & 
      selection_type != "allallall" & 
      selection_type != "allsome" & 
      selection_type != "somesome", na.rm=TRUE) %>% #this is filtering out the strange repeats
  dplyr::filter(english >= 0.75, na.rm=TRUE) %>% # excluding less than or equal to 75% english exposure
  dplyr::filter(english != "NA", na.rm=TRUE)%>% #avoiding the missing value that snuck back in
  dplyr::mutate(age_round = round(age, digits = 2))%>% #for easier grouping
  dplyr::mutate(agesplit = cut(age_round, breaks=c(3, 3.5, 4, 4.5, 5, 6.5)),
   agesplit = factor(agesplit,
         labels=c("3-3.5 years", "3.5-4 years", 
            "4-4.5 years", "4.5-5 years", 
            "5-6.5 years")))%>% #for better-looking graphs
  dplyr::filter(agesplit != "NA", na.rm = TRUE) #just in case any NAs popped up

#for computing means
df$correct %<>%
  str_replace("Y", 1)%>%
  str_replace("N", 0)

#renaming for better graphs at the start
df$trial_type %<>%
  str_replace("all", "All")%>%
  str_replace("some", "Some")%>%
  str_replace("none", "None")
  
df$selection_type %<>%
  str_replace("all", "All")%>%
  str_replace("some", "Some")%>%
  str_replace("none", "None")
```

```{r exclusions}
# Exclusions
df1 <-  subset(df, sub_id != "11716_14" & sub_id != "11316_2" 
     & sub_id != "121815_7" & sub_id != "1616_6" 
     & sub_id != "TM001" & sub_id != "TM003" 
     & sub_id != "TM004" & sub_id != "12416_9" 
     & sub_id!= "TM011") #excluded due to consent, low number of trials, and parental interference

#opaque missing data issue
df <- df1

#for figuring out percentage of trials lost due to RT exclusion
pre_cut <- df
```

```{r rt exclusion}
# qplot(rt, data = df)

#There are some really crazy rts - exclude everything above 15s
df$clean.rt <- df$rt
df$clean.rt[df$rt > 15000] <- NA
mlog <- mean(log(df$clean.rt), na.rm=TRUE)
sdlog <- sd(log(df$clean.rt), na.rm=TRUE)

#Assign "NAs" to clean rts 3 sds outside of log of mean rt
df$clean.rt[log(df$clean.rt) > mlog + 3*sdlog | 
      log(df$clean.rt) < mlog - 3*sdlog] <- NA

#how much data do we lose in this data cleaning?
outlier <- as.numeric(sum(is.na(df$clean.rt) == TRUE))
total_loss <- outlier
percentage <- as.numeric((total_loss/nrow(df))*100)
```

# Introduction

Language comprehension in context is an inferential process. Listeners are not limited to interpreting the literal meaning of speakers' utterances; they can also reason about what the speaker intended, based on alternative utterances. In the case of *pragmatic implicatures* [@grice1975], a speaker employs a weaker literal description to imply that a stronger alternative is true. Adult listeners tend to infer from the statement "I ate *some* of the cookies" that some, but not all, of the cookies remain. This *scalar implicature* (SI) relies heavily on a knowledge of the relevant lexical alternatives in the quantifier scale $<$*some*, *all*$>$. On standard theories, a listener must be able to contrast "some" with the stronger descriptor "all" to compute the implicature [@grice1975; @levinson2000]. 

SIs are challenging for children until surprisingly late in development [@noveck2001]. For example, when judging a scene in which three of three horses have jumped over a fence, five-year-olds are likely to endorse the statement "some of the horses jumped over the fence" as felicitous, despite the availability of a more informative alternative ["all"; @papafragou2003]. Children do seem to have some knowledge of these scalar terms, however; for example, they differntially reward speakers based on the informativeness of their scalar descriptions [@katsos2011]. Given this early sensitivity, why do children still struggle to compute scalar implicatures until fairly late in development? 

One possible cause of children's failures is that they may not have access to relevant lexical alternatives [@barner2010]. This idea, which we will refer to as the *Alternatives Hypothesis*, predicts that if children cannot quickly and reliably bring to mind the relevant alternative quantifiers (e.g., "all" in a situation where they hear "some") they will be unable to compute the implicature. The alternatives hypothesis makes a number of predictions about children's abilities in reasoning about quantifiers, some of which have been confirmed empirically. For example, consistent with the idea of inaccessible alternatives, @barner2011 showed that four-year-olds could not compute the quantifier expression "only some" (which should force alternatives to be negated semantically, rather than pragmatically). In light of this hypothesis, what are the proper alternatives for SIs? 

Here, empirical evidence has been changing rapidly. Although the conventional view on SI is that the primary inferential alternative is "all," a new body of evidence suggests that more alternatives may be necessary. For example, @degen2015 found that set size  can change the felicity of quantifier SIs for adults: "some" is more felicitous when participants could not say "one" or "two." In a computational reanalysis of @degen2015 and other data, @franke2014 showed that a high weight on the alternative "none" was critical for fitting these data. And in a recent study with children, @skordos2016 found that exposing children to either "all" *or* "none" facilitated computation of subsequent SIs. Taken together, these data suggest that the availability of alternatives---particularly "none"---does affect scalar implicature processing. 

This relationship to "none" is unexpected on classic Gricean theories [@grice1975; @horn1972], where the only alternatives should be those logically entailed by the original message (i.e. "all"). But it *is* in fact predicted by recent probabilistic models of implicature. Under these models, all the relevant alternatives compete with one another [@goodman2013; @franke2014]. On the other hand, all of the evidence cited above for the claim of "none" as an alternative is relatively indirect, and such a substantial revision to theory requires further evidence. 

One other recent developmental study further supports the importance of "none" in SIs and provides the starting point for our current experiment. Horowitz \& Frank [-@horowitz2015] designed a referent selection paradigm that could be used across a broad age range (3--5 years) to explore both scalar and ad-hoc (context-dependent) implicatures. In this task, children saw three book covers, each featuring four familiar objects (Figure \ref{fig:image}). On target trials, the experimenter described a book using a semantically ambiguous description (e.g., "On the cover of my book, some of the pictures are cats" [scalar] or "On the cover of my book are cats" [ad hoc]). Children succeeded on ad-hoc trials but largely failed to make SIs, suggesting they had the pragmatic competence necessary to compute the implicature, but failed to do so for scalar descriptors. 

Interestingly, in Horowitz \& Frank [-@horowitz2015], the same children who failed on SI also failed on unambiguous "none" control trials (e.g., "On the cover of my book, *none* of the pictures are cats"")---and in several samples, performance was highly correlated between "none" and "some" trials. This result would be predicted if "none" were in fact an inferential alternative to "some." If children were not computing its semantics appropriately in an online fashion, they would fail in the "some" SI computation as well, leading to a correlation.

```{r image, fig.pos = "b", fig.align='center', fig.width=2.8, fig.height=2.8, fig.cap = "Example trial stimuli used in Horowitz and Frank (2015)."}
img <- png::readPNG("figs/implicatures_demo_letters.png")
grid::grid.raster(img)
```

One further prediction of the alternatives hypothesis relates to processing time. Perhaps children who have a fully-established quantifier scale---and hence can make correct SIs---take additional time in using this information, due to competition between alternatives. Congruent with this prediction, our intuition in Horowitz \& Frank [-@horowitz2015] and Horowitz, Schneider \& Frank [-@horowitzInPrep] was that when children made correct SIs they appeared to be taking longer than when they failed. Motivated by this observation, we adapted our SI task for the iPad to collect detailed and accurate developmental reaction time data. Although reaction time measures have been commonplace in studies of adults' SI processing, they have been almost entirely absent in the developmental literature [with the exception of @huang2009, whose data showed little evidence of SI computation]. 

Thus, in our current study, we explore children's response latencies in an iPad adaptation of the Horowitz \& Frank [-@horowitz2015] SI task. In our analyses, we examine overall accuracy and patterns of performance, as in Horowitz \& Frank [-@horowitz2015], and find that children not only struggle in making SIs, but replicate the finding that they have difficulty with "none" until fairly late in development. Congruent with our predictions, in reaction time analyses we find evidence of a speed-accuracy trade-off for both quantifiers, such that children who succeed exhibit longer response latencies. Finally, we use a Drift Diffusion Model [@ratcliff1978; @ratcliff1998] to explore the source of this increased reaction time. Overall, our findings are consistent with a version of the Alternatives Hypothesis under which "none" is an important inferential alternative in SI and its availability causes slower processing times but correct SIs.

# Method

We adapted the scalar implicature paradigm developed by @horowitz2015 for the iPad. In addition to capturing reaction time (RT) data, this version included more trials, and standardized prosody across all trials, as well as a randomized design.\footnote{The full experiment can be viewed online at \texttt{https://rosemschneider.github.io/tablet\_exp/si\_tablet.html} and all of our data, processing, experimental stimuli, and analysis code can be viewed in the version control repository for this paper at: \texttt{https://github.com/rosemschneider/SI\_tablet}.} 

## Participants

```{r descriptives}
#sex information
num_gender <- df%>%
  select(sub_id, sex)%>%
  distinct()%>%
  group_by(sex)%>%
  summarise(n=n())

f <- num_gender %>%
  filter(sex == "F")

m <- num_gender %>%
  filter(sex == "M")

#number of trials for each participant in each agegroup
num_trials <- df %>%
  group_by(sub_id, agesplit)%>%
  summarise(n = n())

#age information - summary statistics, broken down by age
num_kids <- df %>% 
  dplyr::select(sub_id, agesplit, age) %>%
  dplyr:: distinct()%>%
  dplyr::group_by(agesplit)%>%
  dplyr::summarize(n=n(), mean=mean(age), sd=sd(age), median=median(age))%>%
  dplyr::mutate(total.n = sum(n))

num_young3s <- num_kids %>%
  filter(agesplit == "3-3.5 years")

num_old3s <- num_kids %>%
  filter(agesplit == "3.5-4 years")

num_young4s <- num_kids %>%
  filter(agesplit == "4-4.5 years")

num_old4s <- num_kids %>%
  filter(agesplit == "4.5-5 years")

num_5s <- num_kids %>%
  filter(agesplit == "5-6.5 years")
```

\begin{table}[t]
\centering
\begin{tabular}{c c c c c } 
 \hline
 Age group & N & Mean & Median & SD \\
 \hline
 3--3.5 years & `r num_young3s$n` & `r round(num_young3s$mean, 2)` & `r round(num_young3s$median, 2)` & `r round(num_young3s$sd, 2)`\\
 3.5--4 years & `r num_old3s$n` & `r round(num_old3s$mean, 2)` & `r round(num_old3s$median, 2)` & `r round(num_old3s$sd, 2)` \\ 
 4--4.5 years & `r num_young4s$n` & `r round(num_young4s$mean, 2)` & `r round(num_young4s$median, 2)` & `r round(num_young4s$sd, 2)`\\
 4.5--5 years & `r num_old4s$n` & `r round(num_old4s$mean, 2)` & `r round(num_old4s$median, 2)` & `r round(num_old4s$sd, 2)` \\
 5--6.5 years & `r num_5s$n` & `r round(num_5s$mean, 2)` & `r round(num_5s$median, 2)` & `r round(num_5s$sd, 2)` \\
 \hline
\end{tabular}
\caption{Age information for all participants.}
\label{tab:age}
\end{table}

Table \ref{tab:age} shows the breakdown of participant age information. Included are `r num_young3s$total.n` children out of a planned sample of 120 participants, recruited from both a local daycare and children's museum. `r num_english$n` additional children were excluded from analysis based on planned exclusion criteria of low English language exposure ($\leq 75\%$) or $<50\%$ of trials completed. Included in our sample were `r f$n` females and `r m$n` males.^[Based on Horowitz \& Frank [-@horowitz2015], we initially planned to collect data from children aged 3--5 years. After 57 participants, however, we observed significantly lower performance on implicature trials across all age groups, indicating that the iPad scalar implicature task was slightly more challenging, and included an older age group of 24 5--6.5-year-olds.]

## Stimuli and design

The general format of the task was identical to Horowitz \& Frank [-@horowitz2015], with the exception of added items for additional trials. The study was programmed in HTML, CSS, and JavaScript, and displayed to children on a full-sized iPad. Each trial displayed three book covers, each containing a set of four familiar objects (Figure \ref{fig:image}).  Each session involved 30 trials, with 10 trials per quantifier ("all", "some", and "none"). Each audio clip used the same three initial sentence frames (e.g., "On the cover of my book, *some* of the pictures...") to emphasize prosody equally across all trials. The average length of each clip (including target item phrase, e.g., "...are cats") was approximately 6s. Quantifier triad order, items (within category), target item, and quantifier were randomized for all participants. There were 270 different target items and audio clips. 

## Procedure

Sessions took place individually in a small testing room away from the museum floor or the classroom of the daycare. To familiarize children with the iPad, each session began with a "dot game," which required them to press dots on the screen as fast as possible. After the dot game, the experimenter introduced them to "Hannah," a cartoon character who wanted to play a guessing game with her books. The experimenter explained that Hannah would show the child three books, and would give one hint about which book she had in mind, so they had to listen carefully. Children then saw a practice trial with an unambiguous noun referent. 

Each trial allowed 2.5s for children to visually inspect the book covers before the prompt played (e.g., "On the cover of my book, *none* of the pictures are cats."). Reaction times were measured from the onset of the target word. Children could only make one selection. If a child did not hear Hannah's prompt, the experimenter repeated it, matching the original prosody. Once children made their selection, a green box appeared around the chosen book. The experiment was self-paced, and children initiated each trial by pressing a button that appeared after they made their selection.

```{r overall_acc, fig.pos = "t", fig.width=3.2, fig.height=2, fig.cap = "Children's overall accuracy for each quantifier type. Bars show mean performance for each age group. Error bars are 95-percent confidence intervals."}
#get mean performance for each trial type, along with 95% CIs
df %<>%
  dplyr::mutate(correct = as.numeric(correct))

ms <- df %>%
  group_by(trial_type, agesplit)%>%
  multi_boot_standard("correct", na.rm=TRUE)

ms %<>%
  rename(Age = agesplit)

#Plot bar graph of mean performance, with 95% CIs for trial type split by age
ggplot(data = ms, 
   aes(x=trial_type, y=mean, fill=trial_type, alpha = Age)) +
  geom_bar(stat="identity", position = position_dodge()) +
  geom_linerange(aes(ymin = ci_lower,
        ymax = ci_upper),
      size = .4,
      show.legend = FALSE,
       position=position_dodge(.9)) +
  ylab("Proportion correct") + 
  xlab("Trial Type") +
  theme_bw(base_size = 7) + 
  scale_fill_solarized(name = "Age") + 
  scale_alpha_manual(values=c(.4, .5,.625,.75,1)) +
  theme(axis.title.x = element_text(size=7),
        legend.position=c(.8,.8), 
        legend.key.size=unit(.2, "cm"),
        axis.title.y  = element_text(size=7),
        legend.title=element_text(size=5), 
        legend.text=element_text(size=5), 
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  guides(fill = FALSE)
```
# Results
For trials where the child had missed the prompt or was not paying attention, we excluded reaction times (RTs) longer than 15s. After this initial cut, we excluded RTs outside three standard deviations of the log of mean reaction time. This cleaning resulted in RT data loss for `r total_loss` trials (`r round(percentage, 2)`%). We observed a fairly wide RT distribution for all trial types. This variability in children's RTs may have been exaggerated by testing in a museum environment, on in the motoric demands associated with making a response on a tablet [@franktablet]. "Success" is defined as choosing the image consistent with the scalar description (e.g., selecting the book with two cats and two birds in response to the prompt "On the cover of my book, *some* of the pictures are cats.").

## Accuracy

```{r t tests}
#Tests of performance
#aggrgate 
t_tests <- aggregate(correct ~ trial_type + agesplit +  sub_id, data=df, mean)

#tests against quantifiers by age and quantifier type

#3-3.5s all and none
young3_allnone <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="All")$correct, var.equal = TRUE)
#all and some
young3_allsome <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="All")$correct, var.equal = TRUE)
#some and none
young3_somenone <- t.test(subset(t_tests, agesplit=="3-3.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3-3.5 years" & trial_type=="Some")$correct, var.equal = TRUE)

#3.5s-4s 
old3_allnone <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="All")$correct, var.equal = TRUE)
old3_allsome <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="All")$correct, var.equal = TRUE)
old3_somenone <- t.test(subset(t_tests, agesplit=="3.5-4 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="3.5-4 years" & trial_type=="Some")$correct, var.equal = TRUE)

#4-4.5s
young4_allnone <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="All")$correct, var.equal = TRUE)
young4_allsome <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="All")$correct, var.equal = TRUE)
young4_somenone <- t.test(subset(t_tests, agesplit=="4-4.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4-4.5 years" & trial_type=="Some")$correct, var.equal = TRUE)

#4.5-5s
old4_allnone <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="All")$correct, var.equal = TRUE)
old4_allsome <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="All")$correct, var.equal = TRUE)
old4_somenone <- t.test(subset(t_tests, agesplit=="4.5-5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="4.5-5 years" & trial_type=="Some")$correct, var.equal = TRUE)

#5s
fives_allnone <- t.test(subset(t_tests, agesplit=="5-6.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="5-6.5 years" & trial_type=="All")$correct, var.equal = TRUE)
fives_allsome <- t.test(subset(t_tests, agesplit=="5-6.5 years" & trial_type=="Some")$correct, subset(t_tests, agesplit=="5-6.5 years" & trial_type=="All")$correct, var.equal = TRUE)
fives_somenone <- t.test(subset(t_tests, agesplit=="5-6.5 years" & trial_type=="None")$correct, subset(t_tests, agesplit=="5-6.5 years" & trial_type=="Some")$correct, var.equal = TRUE)

```

```{r incorrect choices}
#for incorrect selection analyses
wrong <- df %>%
  filter(correct == "0")%>%
  group_by(trial_type, selection_type, agesplit)%>%
  dplyr::summarise(n=n())%>%
  mutate(n.total = sum(n), prop = n/n.total)
```

Figure \ref{fig:overall_acc} shows children's mean performance for each trial type, split by age group. For each age group, we saw significantly lower accuracy for the quantifiers "some" and "none" in comparison to "all" (all $p$s $< .01$ in two-sample t-tests for each age group). These results generally replicate our previous findings using this paradigm [@horowitz2015]; one difference from previous results was in implicature trials. Children aged 3--5 years performed significantly lower on "some" (implicature) trials in this task in comparison to @horowitzInPrep ($p < .01$ for all tests). Thus, while the iPad adaptation was generally successful, implicatures were more difficult, perhaps because of the non-social nature of the iPad interaction. 

```{r comparison}
#Testing this data against data from Horowitz, Schneider, & Frank (in prep.) exp. 3 SI task
d_si <- read.csv("experiment3.csv") #pull in aggregated data from previous analysis

t_tests %<>%
  spread(trial_type, correct)
#only for some and none trials, because we don't find that the all trials are much affected by the transfer to the iPad
#3-3.5 years
list <- c(
t.test(subset(t_tests, agesplit=="3-3.5 years")$None, subset(d_si, Age=="3-3.5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="3-3.5 years")$Some, subset(d_si, Age=="3-3.5 years")$Some, var.equal = TRUE),

#3.5-4s
t.test(subset(t_tests, agesplit=="3.5-4 years")$None, subset(d_si, Age=="3.5-4 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="3.5-4 years")$Some, subset(d_si, Age=="3.5-4 years")$Some, var.equal = TRUE),

#4-4.5s
t.test(subset(t_tests, agesplit=="4-4.5 years")$None, subset(d_si, Age=="4-4.5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="4-4.5 years")$Some, subset(d_si, Age=="4-4.5 years")$Some, var.equal = TRUE),

#4.5-5s
t.test(subset(t_tests, agesplit=="4.5-5 years")$None, subset(d_si, Age=="4.5-5 years")$None, var.equal = TRUE),
t.test(subset(t_tests, agesplit=="4.5-5 years")$Some, subset(d_si, Age=="4.5-5 years")$Some, var.equal = TRUE))
```

```{r diptest, fig.pos = "t", fig.width=3.2, fig.height=2, fig.cap = "Frequency histogram of participant totals for each trial type, across all participants."}
#Diptest for bimodal performance on quantifiers
#mean performance and 95% cis
ms <- df %>%
  group_by(sub_id, trial_type) %>%
  multi_boot_standard("correct", na.rm=TRUE)

diptest_all <- diptest::dip.test(filter(ms, trial_type == "All")$mean)
diptest_some <- diptest::dip.test(filter(ms, trial_type == "Some")$mean)
diptest_none <- diptest::dip.test(filter(ms, trial_type == "None")$mean)

#Frequency histogram of selections on trials
ms <- df %>%
  group_by(sub_id, trial_type) %>%
  summarise(correct = sum(correct))
  
# quartz()
ggplot(ms, aes(x = correct, fill=trial_type)) + 
  geom_histogram(binwidth = 1) + 
  theme_bw(base_size = 7) + 
  xlab("Total correct responses per participant") + 
  ylab("Frequency") + 
  facet_wrap(~trial_type) + 
  theme(legend.position="none") + 
  theme(axis.title.x = element_text(size = 7), axis.title.y = element_text(size = 7)) +
  scale_x_continuous(breaks = c(0,2,4,6,8,10)) + 
  langcog::scale_fill_solarized()
```

```{r dense, fig.env = "figure*", fig.pos = "t", fig.width=6.2, fig.height=2.6, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Density plots of reaction times for correct and incorrect responses on each trial type, split by age."}
trialtypes <- c("all", "some", "none")

#For easier to read graph
df$resp1 <- df$resp %>%
  str_replace("lower", "Incorrect")%>%
  str_replace("upper", "Correct")

#Stats for median lines
rt_stats <- df %>%
  group_by(agesplit, trial_type, resp1)%>%
  summarise(mean=mean(rt), median = median(rt))%>%
  mutate(m = median/1000)

m_upper <- rt_stats %>%
  filter(resp1 == "Correct")

m_lower <- rt_stats %>%
  filter(resp1 == "Incorrect")

#for easier to read graph
df$resp1 <- factor(df$resp1, levels=c("Incorrect", "Correct"))

# quartz()
ggplot(df, aes(x=q)) + 
  geom_density(aes(weight = sum(resp1=="Correct")/length(resp1), fill = resp1), alpha=0.5) +
  facet_grid(trial_type ~ agesplit) + 
  scale_fill_solarized(name = "Response") +
  xlab("Response time (s)") + 
  ylab("Density of responses") + 
  xlim(c(0,10)) + 
  theme_bw(base_size = 7) + 
  theme(axis.title.x = element_text(size=7),
    axis.title.y  = element_text(size=7),
    legend.key.size=unit(.5, "cm"),
    legend.title=element_text(size=6.5),
    legend.text=element_text(size=6))
```

```{r accuracy model}
lm <- summary(glmer(correct ~ age * trial_type + 
     (trial_type | sub_id), 
     family = "binomial", data = df, control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5))))
```

We next fit a logistic mixed effects model predicting correct response as an interaction of age and trial type, with random effects of trial type and participant.\footnote{All mixed effects models were fit in \texttt{R} using the \texttt{lme4} package. The model specification was: \texttt{correct $\sim$ age * trial type + (trial type | subject id)}.} Performance was significantly lower on "some" ($\beta = `r round(lm$coefficients[4], 2)`$, $p < .0001$) and "none" trials ($\beta = `r round(lm$coefficients[3], 2)`$, $p < .0001$). There was also a significant interaction between age and trial type on "none" trials ($\beta$ = `r round(lm$coefficients[5], 2)`, $p$ = .0005), indicating that children's performance with this difficult quantifier increased with age. This model also showed that children's performance showed a trend towards significance for "some" trials ($\beta$ = `r round(lm$coefficients[6],2)`, $p$ = .051). 

```{r correlation}
ms.acc <- df %>%
  dplyr::group_by(trial_type, agesplit, sub_id) %>%
  multi_boot_standard("correct", na.rm = TRUE) %>%
  dplyr::select(-ci_lower, -ci_upper)%>%
  spread(trial_type, mean)

#correlation test
sn_cor <- cor.test(ms.acc$Some, ms.acc$None)

r_corr <- round(sn_cor$estimate, digits = 2)
p_corr <- round(sn_cor$p.value, digits = 6)
```

```{r trials correct}
#How many children got the majority of trials correct? 
perc <- df %>%
  group_by(sub_id, trial_type, correct)%>%
  summarise(n=n())%>%
  group_by(trial_type)%>%
  filter(correct == 1, trial_type == "Some" | trial_type == "None")%>%
  filter(n >= 9)

none_perc <- perc %>%
  filter(trial_type == "None")

some_perc <- perc %>%
  filter(trial_type == "Some")
```

Figure \ref{fig:diptest} shows distributions of correct responses for all trial types. Performance on "some" and "none" trials was bimodal (Hartigan's $D$ = `r round(diptest_some$statistic, 2)`, $p$ < .0001) and "none" trials ($D$ = `r round(diptest_none$statistic, 2)`, $p$ < .0001). While children's average accuracy was low for these quantifiers, there were some children who were correct on the majority of these trials ("Some": N = `r nrow(some_perc)`; "None": N = `r nrow(none_perc)`) and the others were typically incorrect on the majority of trials. Children did not appear to be responding randomly. As in previous work, we found a strong correlation between children's accuracy on "some" and "none" trials ($r$ = `r r_corr`, $p$ < .0001). Children also exhibited some interesting systematicity in their errors: on incorrect "some" trials they overwhelmingly chose "all," while on "none" trials they also chose "some" at about half the rate of "all." 

## Reaction time

```{r rt summary stats}
rt_sstats <- df %>%
  gather(measure, rt, rt, clean.rt) %>%
  group_by(trial_type, measure) %>%
  summarise(mean = mean(rt, na.rm=TRUE), 
    sd = sd(rt, na.rm=TRUE), 
    max = max(rt, na.rm=TRUE), 
    min = min(rt, na.rm=TRUE), 
    median=median(rt, na.rm=TRUE)) 
```

```{r rt corr}
#correlation between reaction time and age
age_corr <- cor.test(df$age, df$rt)
```

```{r rt corr_1}
ms.rt <- df %>%
  dplyr::group_by(trial_type, agesplit, sub_id) %>%
  dplyr::summarise(rt = mean(clean.rt, na.rm=TRUE)) %>%
  spread(trial_type, rt) 
# 
# ggcorplot(ms.rt %>% filter(complete.cases(ms.rt)) %>% dplyr::select(None, Some, All))

allsome_corr <- cor.test(ms.rt$All, ms.rt$Some)
allnone_corr <- cor.test(ms.rt$All, ms.rt$None)
somenone_corr <- cor.test(ms.rt$Some, ms.rt$None)
```

```{r rt_acc_model}
#predicting rt as an interaction of trial numb, age, type, accuracy, with random effects of trial type and subject 
rt_acc_lm <- summary(lmer(log(clean.rt) ~ log(trial_num) + 
     scale(age, scale=FALSE, center = TRUE) * trial_type * correct + 
    (trial_type | sub_id), 
    data = df))
```

```{r rt_model}
#predicting RTs as interaction of trial number, age, trial type, with random effects of trial type and subject
rt_lm <- summary(lmer(log(clean.rt) ~ log(trial_num) + 
     scale(age, scale=FALSE, center = TRUE) * trial_type + 
     (trial_type | sub_id), 
     data = filter(df, correct==1)))
```

We fit a linear mixed effects model predicting log RT on correct trials as a function of log trial number, the interaction of age and trial type, and random effects of trial type by subject.^[Model specification: \texttt{log(reaction time) $\sim$ log(trial number) + age * trial type + (trial type | subject id)}. Age was centered for ease of interpretation of coefficients, and we calculated \emph{p} values via the $t=z$ approximation.] Reaction times were longer on "none" ($\beta = `r round(rt_lm$coefficients[4], 2)`$, $p$ < .0001) and "some" trials ($\beta = `r round(rt_lm$coefficients[5], 2)`$, $p$ < .0001), and reaction times decreased with age ($\beta = `r round(rt_lm$coefficients[3], 2)`$, $p$ < .0001). There were no significant interactions between age and trial type. The model also showed a main effect of trial number, with reaction times decreasing over the course of the study ($\beta$ = `r round(rt_lm$coefficients[2], 2)`, $p$ < .0001). 

Examination of the pattern in Figure \ref{fig:dense} suggests that accuracy and reaction time may interact, however. In particular, while correct responses on "all" trials appear to be faster than the (few) incorrect responses, the opposite is true for "none" and "some" trials: Errors have faster RTs, potentially indicating a speed-accuracy trade-off. To test for this effect, we fit another mixed effects model, this time including accuracy and its interactions with age and trial type as predictors. This model revealed that correct trials overall had faster RTs ($\beta = `r round(rt_acc_lm$coefficients[6], 2)`$, $p = .0002$), but that this accuracy term interacted negatively with trial type such that both "none" and "some" trials had slower RTs for correct trials ($\beta = `r round(rt_acc_lm$coefficients[10], 2)`$, $p < .0001$; $\beta = `r round(rt_acc_lm$coefficients[11], 2)`$, $p < .0001$). There were no three-way interactions of trial-type and age. This model thus provides evidence of a speed-accuracy trade-off for "some" and "none" trials. 

## Drift diffusion models

```{r developmental parameter estimates}
#first, filter any outlier RTs - otherwise, RWiener will not fit the model
df %<>%
  filter(clean.rt != "NA")

trialtypes = c("All", "Some", "None")

#create dataframe
sub.pars <- data.frame(Separation = numeric(),
                       Non.Decision = numeric(),
                       Bias = numeric(),
                       Drift = numeric(),
                       Trial.Type = character(),
                       SubID = character(), 
                       Age = character())
sub.pars$Trial.Type <- as.character(sub.pars$Trial.Type)
sub.pars$SubID <- as.character(sub.pars$SubID)
sub.pars$Age <- as.character(sub.pars$Age)

temp.pars <- sub.pars

df$resp <- as.character(df$resp)

#this takes a while to run, but estimating parameters for each subject
subs <- unique(df$sub_id)

for (j in 1:length(subs)) {
  sid <- as.character(subs[j]) 
  for (i in 1:length(trialtypes)) {
    ttype <- as.character(trialtypes[i])
    dat <- as.data.frame(subset(df, trial_type == ttype & sub_id == sid))
    opt <- optim(c(1, .1, .1, 1), wiener_deviance, 
                 dat=dplyr::select(dat, c(q, resp)), method="Nelder-Mead")
    pars <- c(opt$par, ttype, sid, dat$agesplit[1])
    temp.pars[i,] <- pars
  }
  sub.pars <- rbind(temp.pars, sub.pars)
} 
```

```{r developmental parameter df}
#create df for parameter estimates
sub.pars$Separation <- as.numeric(sub.pars$Separation)
sub.pars$Non.Decision <- as.numeric(sub.pars$Non.Decision)
sub.pars$Bias <- as.numeric(sub.pars$Bias)
sub.pars$Drift <- as.numeric(sub.pars$Drift)


sub.pars <- select(sub.pars, -Age) %>%
  rename(sub_id = SubID) %>%
  left_join(df %>% 
              group_by(sub_id) %>% 
              distinct %>% 
              select(sub_id, age_round, agesplit)) %>%
  gather(Param, Value, Separation:Drift) 


sub.pars.ms <- sub.pars %>%
  group_by(agesplit) %>%
  mutate(Age = mean(age_round)) %>%
  group_by(agesplit, Age, Trial.Type, Param) %>%
  multi_boot_standard("Value", na.rm = TRUE)

sub.pars.dev <- sub.pars
sub.pars.ms.dev <- sub.pars.ms
```

```{r devo param models}
#models - predicting parameter value as interaction of age, trial type
#separation boundary
devo_sep_lm <- summary(lmer(Value ~ age_round * Trial.Type + (1 | sub_id), 
             data = filter(sub.pars.dev, Param=="Separation")))
#non-decision
devo_nd_lm <- summary(lmer(Value ~ age_round * Trial.Type + (1 | sub_id), 
             data = filter(sub.pars.dev, Param=="Non.Decision")))
#bias
devo_bias_lm <- summary(lmer(Value ~ age_round * Trial.Type + (1 | sub_id), 
             data = filter(sub.pars.dev, Param=="Bias")))
#drift
devo_drift_lm <- summary(lmer(Value ~ age_round * Trial.Type + (1 | sub_id), 
             data = filter(sub.pars.dev, Param=="Drift")))
```

Motivated by the evidence of a speed-accuracy trade-off, we further explored the interaction between reaction time and accuracy in more depth using drift diffusion modeling (DDM). RTs associated with "some" and "none" indicated that longer RTs were associated with higher accuracy, but what components of the decision process contribute to this? DDM can be used in behavioral tasks to provide a more detailed view of the relationship between accuracy and reaction time [@ratcliff1998]. In DDM, a behavioral response (a correct or incorrect choice) is the result of noisy data accumulation through a diffusion process. Responses have *separation boundaries* that are dependent on the amount of information needed to initiate a response, and *drift rate* formalizes the rate of data accumulation. *Nondecision* is the amount of time between stimuli offset and initiating the diffusion process (i.e., encoding). Finally, different responses may be *biased* in their starting point in the diffusion process. Thus, a DDM can reveal more differences in the SI decision-making process, and provide clues about the causes underlying this speed-accuracy trade-off.

### Developmental analyses

Although DDMs are traditionally fit to data from two-alternative forced-choice tasks, here we estimate the drift process between a correct and incorrect choice, with two options in each trial being "incorrect," and only one being consistent with the target noun and quantifier. We estimated parameters for each subject for each trial type using the \texttt{RWiener} package. We then aggregated across subjects to obtain means and confidence intervals for each age group. Figure \ref{fig:devo_param_plot} shows the parameter estimates for each age group, split by trial type. To fit the model, we excluded `r outlier` trials with outlier RTs.

```{r devo_param_plot, fig.env = "figure*", fig.pos = "t", fig.width=6.6, fig.height=1.45, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Parameter estimates for drift diffusion model, split by age and trial type. Error bars are 95 percent confidence intervals computed by nonparametric bootstrap."}
quartz()
ggplot(sub.pars.ms.dev, aes(x=Age, y=mean, color=Trial.Type)) +
  geom_point(size = 1.2, position=position_dodge(.05)) + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                 position=position_dodge(.05)) +
  geom_line(aes(colour = Trial.Type, group=Trial.Type)) + 
  facet_wrap(~Param, scales="free", ncol = 4) + 
  theme_bw(base_size = 7.7) + 
  theme(axis.text.x = element_text(size = 7),
        axis.title.x = element_text(size = 7.5),
        axis.title.y = element_text(size = 7.5),
        axis.text.y = element_text(size = 7),
        legend.text = element_text(size = 6), 
        legend.title = element_text(size = 6.5)) + 
  langcog::scale_color_solarized(guide_legend(title="Trial Type")) + 
  ylab("Mean") + xlab("Age (years)")
```

For each parameter estimate, we ran a mixed effects model, predicting parameter value as an interaction of age and trial type.\footnote{The specifications for all parameter models are as follows: \texttt{Parameter Value $\sim$ age * trial type + (1 | subject ID)}} There was no significant effect of trial type in boundary separation, indicating that roughly the same amount of information is needed to make a decision for each quantifier. This should be expected, given our experimental design. For non-decision time, we found a significant main effect of age ($\beta$ = `r round(devo_nd_lm$coefficients[2], 2)`, $p$ < .00001), as well as an interaction between age and "none" trials ($\beta$ = `r round(devo_nd_lm$coefficients[5], 2)`, $p$ = .01). As expected in drift rate, there was a negative main effect of trial type ("None": $\beta$ = `r round(devo_drift_lm$coefficients[3], 2)`, $p$ = .0185; "Some": $\beta$ = `r round(devo_drift_lm$coefficients[4], 2)`, $p$ = .03). Interestingly, for bias there was a significant negative effect of "none" trials ($\beta$ = `r round(devo_bias_lm$coefficients[3], 2)`, $p$ = .0005), and "some" trials trended towards significance ($\beta$ = `r round(devo_bias_lm$coefficients[4], 2)`, $p$ = .0503), as well as a significant interaction between age and "none" trials ($\beta$ = `r round(devo_bias_lm$coefficients[5],2)`, $p$ = .023). 

In sum, the parameter estimates from our DDM align with the analyses presented above: Older children are more likely to respond correctly in our scalar implicature task, while younger children's failures appear to be due to a low rate of data accumulation and a high separation boundary. One hypothesis for older children's successes in our task, and their trend towards more positive drift rates and lower biases, is rooted in a firmer grasp of the quantifier scale. In @horowitzInPrep, we observed a significant correlation between children's performance on a quantifier-knowledge task and SI computation. We found that this correlation increased with age, and that difficulties in computing an SI may be grounded in quantifier comprehension. It is possible that older children's SI success, and the observed speed-accuracy tradeoff, may reflect increased knowledge of available lexical alternatives---particularly "none"---and the cost of comparing contrasting quantifiers to make SIs.

### Exploratory analyses

We also conducted an exploratory analysis, examining differences in the decision-making process for children who consistently made SIs compared with those who did not. This exploratory analysis was motivated by the hypothesis that successful participants in this task might display different signatures in their decision processes. Therefore, we split children into two groups by accuracy on scalar implicature trials, and then estimated parameters by accuracy group. High accuracy was defined as an average of 75\% or higher performance on scalar implicature trials. Figure \ref{fig:param_plot} shows parameter estimates for each accuracy group, split by trial type.

```{r acc param}
#Finding out who has high performance on SI trials
#Manually added in csv
df %<>%
  dplyr::mutate(correct = as.integer(correct))

ms.acc <- df %>%
  group_by(sub_id, trial_type) %>%
  multi_boot_standard("correct", na.rm=TRUE)%>%
  select(-ci_lower, -ci_upper)%>%
  spread(trial_type, mean)%>%
  filter(Some >= .75)%>%
  arrange(sub_id)

#check to make sure that data entered manually equals above
ms.check <- df %>%
  filter(accuracy == "high")%>%
  distinct(sub_id)%>%
  select(sub_id)%>%
  arrange(sub_id)

library(compare)
comparison <- compare(ms.acc$sub_id, ms.check$sub_id, allowAll = TRUE)
#compare shows all sub_ids in common

trialtypes = c("All", "Some", "None")

# #create dataframe for high accuracy
sub.pars.high <- data.frame(Separation = numeric(),
         Non.Decision = numeric(),
         Bias = numeric(),
         Drift = numeric(),
         Trial.Type = character(),
         SubID = character(), 
         Age = character())
sub.pars.high$Trial.Type <- as.character(sub.pars.high$Trial.Type)
sub.pars.high$SubID <- as.character(sub.pars.high$SubID)
sub.pars.high$Age <- as.character(sub.pars.high$Age)

temp.pars.high <- sub.pars.high

# now for low
sub.pars.low <- data.frame(Separation = numeric(),
         Non.Decision = numeric(),
         Bias = numeric(),
         Drift = numeric(),
         Trial.Type = character(),
         SubID = character(), 
         Age = character())
sub.pars.low$Trial.Type <- as.character(sub.pars.low$Trial.Type)
sub.pars.low$SubID <- as.character(sub.pars.low$SubID)
sub.pars.low$Age <- as.character(sub.pars.low$Age)

temp.pars.low <- sub.pars.low

df$resp <- as.character(df$resp)

#this takes a while to run, but estimating parameters for each subject

#make high and low df for indepdendent parameter estimates - bind together below
param.high <- df %>%
  filter(accuracy == "high")

param.low <- df %>%
  filter(accuracy == "low")

subs.high <- unique(param.high$sub_id)
subs.low <- unique(param.low$sub_id)

# first estimate params for high accuracy kids
for (j in 1:length(subs.high)) {
  sid <- as.character(subs.high[j]) 
  for (i in 1:length(trialtypes)) {
  ttype <- as.character(trialtypes[i])
  dat <- as.data.frame(subset(df, trial_type == ttype & sub_id == sid))
  opt <- optim(c(1, .1, .1, 1), wiener_deviance, 
       dat=dplyr::select(dat, c(q, resp)), method="Nelder-Mead")
  pars <- c(opt$par, ttype, sid, dat$agesplit[1])
  temp.pars.high[i,] <- pars
  }
  sub.pars.high <- rbind(temp.pars.high, sub.pars.high)
} 

# now for low
for (j in 1:length(subs.low)) {
  sid <- as.character(subs.low[j]) 
  for (i in 1:length(trialtypes)) {
  ttype <- as.character(trialtypes[i])
  dat <- as.data.frame(subset(param.low, trial_type == ttype & sub_id == sid))
  opt <- optim(c(1, .1, .1, 1), wiener_deviance, 
       dat=dplyr::select(dat, c(q, resp)), method="Nelder-Mead")
  pars <- c(opt$par, ttype, sid, dat$agesplit[1])
  temp.pars.low[i,] <- pars
  }
  sub.pars.low <- rbind(temp.pars.low, sub.pars.low)
} 
```

```{r parameter df}
#bind two param estimates together for high and low
sub.pars <- rbind(sub.pars.low, sub.pars.high)
# sub.pars <- sub.pars.high

#high
sub.pars$Separation <- as.numeric(sub.pars$Separation)
sub.pars$Non.Decision <- as.numeric(sub.pars$Non.Decision)
sub.pars$Bias <- as.numeric(sub.pars$Bias)
sub.pars$Drift <- as.numeric(sub.pars$Drift)

sub.pars <- select(sub.pars, -Age) %>%
  rename(sub_id = SubID) %>%
  left_join(df %>% 
      group_by(sub_id) %>% 
      distinct %>% 
      select(sub_id, age_round, accuracy)) %>%
  gather(Param, Value, Separation:Drift) 

sub.pars.ms <- sub.pars %>%
  group_by(accuracy) %>%
  mutate(Age = mean(age_round)) %>%
  group_by(accuracy, Trial.Type, Param) %>%
  multi_boot_standard("Value", na.rm = TRUE)
```

```{r param_plot, fig.env = "figure*", fig.pos = "!tb", fig.width=6.6, fig.height=1.45, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Parameter estimates for drift diffusion model, split by accuracy and trial type. Error bars are 95 percent confidence intervals computed by nonparametric bootstrap."}

# quartz()
ggplot(sub.pars.ms, aes(x=accuracy, y=mean, color=Trial.Type)) +
  geom_point(size = 1.2, position=position_dodge(.05)) + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
       position=position_dodge(.05)) +
  geom_line(aes(colour = Trial.Type, group=Trial.Type)) + 
  facet_wrap(~Param, scales="free", ncol = 4) + 
  theme_bw(base_size = 7.7) + 
  theme(axis.text.x = element_text(size = 7),
    axis.title.x = element_text(size = 7.5),
    axis.title.y = element_text(size = 7.5),
    axis.text.y = element_text(size = 7),
    legend.text = element_text(size = 6), 
    legend.title = element_text(size = 6.5)) + 
  langcog::scale_color_solarized(guide_legend(title="Trial Type")) + 
  ylab("Mean") + xlab("Accuracy")
```

```{r}
sep_lm <- summary(lmer(Value ~ accuracy * Trial.Type + (1 | sub_id), 
     data = filter(sub.pars, Param=="Separation")))
sep_lmage <- summary(lmer(Value ~ accuracy* Trial.Type *age_round + (1 | sub_id), 
     data = filter(sub.pars, Param=="Separation")))

nd_lm <- summary(lmer(Value ~ accuracy * Trial.Type + (1 | sub_id), 
     data = filter(sub.pars, Param=="Non.Decision")))
nd_lm_age <- summary(lmer(Value ~ accuracy * Trial.Type *age_round + (1 | sub_id), 
     data = filter(sub.pars, Param=="Non.Decision")))

bias_lm <- summary(lmer(Value ~ accuracy* Trial.Type + (1 | sub_id), 
     data = filter(sub.pars, Param=="Bias")))
bias_lmage <- summary(lmer(Value ~ accuracy * Trial.Type * age_round + (1 | sub_id), 
     data = filter(sub.pars, Param=="Bias")))

drift_lm <- summary(lmer(Value ~ accuracy* Trial.Type + (1 | sub_id), 
     data = filter(sub.pars, Param=="Drift")))
drift_lmage <-summary(lmer(Value ~ accuracy* Trial.Type *age_round + (1 | sub_id), 
     data = filter(sub.pars, Param=="Drift")))
```

We again used mixed-effects models to predict DDM coefficients across participants. As in the developmental DDM analysis, there were no significant effects of separation or non-decision. While drift rates showed a significant effect of accuracy, because we estimated parameters for high- and low-accuracy children separately, these differences are expected. 

In our bias estimates, however, we found a significant interaction between accuracy group and trial type on "some" trials ($\beta$ = `r round(bias_lm$coefficients[6], 2)`, $p = .0013$). This interaction suggests that bias (the starting point in the diffusion process) might be an important factor in successfully making a scalar implicature: More successful children were less biased towards incorrect response alternatives, perhaps due to greater knowledge about the quantifier scale.

```{r}
#When we included an age coefficient in this model, this interaction was no longer significant. Instead, we found a significant interaction between age, "some" trials, and accuracy group ($\beta$ = `r round(bias_lmage$coefficients[12], 2)`, $p$ = .024)., suggesting 

#While it seems that when accounting for age children's bias on "some" trials is not affected by scalar implicature accuracy, older children in the low accuracy group are significantly more likely to have a lower bias for "some." This observed interaction, together with the findings of our previous accuracy models, does provide evidence for the availability of "none" providing a salient alternative that results in a speed-accuracy tradeoff in making a scalar implicature.
```

# General Discussion

What makes scalar implicatures using quantifiers so hard for children? The best current hypothesis posits that children do not have access to the appropriate inferential alternatives and hence fail to consider them in their pragmatic computation [@barner2010;@barner2011]. But what are those alternatives? Recent work has suggested that the negative alternative "none" may compete with "some" and "all" when making SIs. Although "none" is not typically considered an alternative in Gricean theories [@grice1975; @horn1972], it nevertheless provides a relevant lexical alternative along the quantifier scale. Our findings here are consistent with this account and provide some additional support. Using a new method, we replicated the pattern found in previous studies that those children who succeed in comprehending the quantifier "none" are also able to make SIs [@horowitz2015;@horowitzInPrep]. In addition, our data revealed a speed-accuracy trade-off, such that reaction times in those trials for which children succeeded in making SIs were slower overall.

One interpretation of this speed-accuracy trade-off is that children who have more inferential alternatives accessible to them (e.g. are considering "none," "some," and "all" together) are both better at making SIs and slower to make them due to the processing cost of making the inference. This theory makes the prediction that children who have greater quantifier knowledge will be more successful in computing SIs, but will display higher RTs due to competing scalar alternatives. Our data are consistent with this account, which is also supported by an exploratory drift diffusion model analysis. We fit a DDM to our data for children who succeeded in making scalar implicatures versus children who failed. The model suggested that bias in "some" and "none" trials might be a key factor related to success---that is, children who were considering "some" and "all" responses equally in their decision were more likely to make the SI. Both of these findings are again consistent with the idea that weighing alternatives appropriately in the SI computation is critical to success. 

The speed-accuracy patterns we report are correlational, however, and other accounts are consistent with these findings as well. For example, some third factor (say inhibitory control) could underlie the ability to succeed in "some" and "none" trials and also explain why some children are able to inhibit their response long enough to complete the SI computation. @horowitzInPrep did not find evidence of correlations between individuals' SI abilities and their executive function using one popular measure (the dimensional change card sort). Other versions of this account (or other accounts entirely) are still possible, however, and should be explored in future work. Nevertheless, our present work suggests that there is a meaningful relationship between children's accuracy and processing times in making scalar implicatures.

# Acknowledgements

Thanks to Bing Nursery School and the San Jose Children's Discovery Museum. Thanks also to Veronica Cristiano, Rachel Walker, and Tamara Mekler for their help with data collection, and to Kara Weisman and Ann Nordmeyer for their assistance creating stimuli. This work was supported by NSF BCS #1456077.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
